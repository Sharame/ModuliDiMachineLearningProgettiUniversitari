{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMm5HBO3hrM5M48zse4zaxU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sharame/Project/blob/main/SyntetichGymDatatset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synthetic Gym Logs Generator (Set-level)\n",
        "Generatore di dataset sintetico per log di allenamento **per serie** (set-level) con:\n",
        "- finestra temporale per utente variabile (end_date tra oggi e +2 anni, durata 2 settimane → 2 anni)\n",
        "- simulazione a stati (fitness/fatigue/skill/resilience)\n",
        "- eventi (skip, injury)\n",
        "- output canonico: `workout_sets.csv`\n",
        "- output derivati: `workoutlogs.csv` (per esercizio), `sessions.csv` (per sessione), `banisterdaily.csv`\n",
        "\n",
        "Obiettivo: produrre dati realistici e poi derivare viste aggregate per i moduli ML.\n"
      ],
      "metadata": {
        "id": "B-sMjqHZhl_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL 1 — (Python) Setup**"
      ],
      "metadata": {
        "id": "3Wxo0kBLhsAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas numpy\n",
        "\n",
        "import os, json, math\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from datetime import date, timedelta, datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "SovnHCMDhz0y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL 2 — (Python) Config**"
      ],
      "metadata": {
        "id": "XbnjzVEqh2RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class CFG:\n",
        "    seed: int = 42\n",
        "    outdir: str = \"data_synth_setlevel\"\n",
        "\n",
        "    n_users: int = 300\n",
        "\n",
        "    # Per-user date ranges\n",
        "    today: date = date.today()\n",
        "    end_date_max_days_ahead: int = 730     # oggi -> +2 anni\n",
        "    min_duration_days: int = 14           # 2 settimane\n",
        "    max_duration_days: int = 730          # 2 anni\n",
        "\n",
        "    # Training schedule\n",
        "    weekly_freq_mu: float = 3.5\n",
        "    weekly_freq_sd: float = 1.0\n",
        "    weekly_freq_min: int = 1\n",
        "    weekly_freq_max: int = 6\n",
        "    weekday_jitter_probs = (0.15, 0.70, 0.15)  # -1,0,+1\n",
        "\n",
        "    # Quantization / realism\n",
        "    load_step: float = 0.25\n",
        "    rpe_step: float = 0.5\n",
        "\n",
        "    # Skip model (baseline per livello)\n",
        "    skip_p0_by_level: dict = None  # lo definiamo sotto\n",
        "    skip_fatigue_weight: float = 0.25\n",
        "    skip_fatigue_cap: float = 1.2\n",
        "    skip_noise_sd: float = 0.10\n",
        "\n",
        "    # quanto la fatica aumenta lo skip (modulatore leggero)\n",
        "    skip_fatigue_weight = 0.25\n",
        "\n",
        "    # cap e rumore (tengono stabile il sistema)\n",
        "    skip_fatigue_cap = 1.2\n",
        "    skip_noise_sd = 0.10\n",
        "\n",
        "    skip_exp_fatigue_scale = 0.85   # quanto l’esperienza \"spegne\" l’effetto fatica (0.0=nessun effetto)\n",
        "\n",
        "    skip_exp_weight: float = 1.2\n",
        "\n",
        "    injury_lambda: float = 0.002   # scala probabilità injury\n",
        "    injury_days_min: int = 7\n",
        "    injury_days_max: int = 28\n",
        "\n",
        "    # Missingness (solo su osservazioni)\n",
        "    p_missing_rpe: float = 0.02\n",
        "    p_missing_load: float = 0.01\n",
        "    p_missing_feedback: float = 0.02\n",
        "\n",
        "    # Banister-like params\n",
        "    tauF_mean: float = 45.0\n",
        "    tauF_sd: float = 8.0\n",
        "    tauD_mean: float = 7.0\n",
        "    tauD_sd: float = 2.0\n",
        "    betaF: float = 0.010\n",
        "    betaD: float = 0.015\n",
        "\n",
        "# Default per skip_p0_by_level\n",
        "if not hasattr(CFG, '__dataclass_fields__') or 'skip_p0_by_level' not in CFG.__dataclass_fields__:\n",
        "    CFG.skip_p0_by_level = {\n",
        "        \"Beginner\": 0.13,\n",
        "        \"Intermediate\": 0.08,\n",
        "        \"Advanced\": 0.05,\n",
        "    }\n",
        "\n",
        "cfg = CFG()\n",
        "\n",
        "cfg.skip_p0_by_level = {\n",
        "    \"Beginner\": 0.13,\n",
        "    \"Intermediate\": 0.08,\n",
        "    \"Advanced\": 0.05,\n",
        "}\n",
        "\n",
        "rng = np.random.default_rng(cfg.seed)\n",
        "\n",
        "OUTDIR = Path(cfg.outdir)\n",
        "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "cfg.today\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojCQRcDBh4al",
        "outputId": "ca8c1d3b-d09f-4809-9597-bc7c166f12a6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.date(2026, 1, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL 3 — (Python) Utils**"
      ],
      "metadata": {
        "id": "46HTer4uh6fO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z: float) -> float:\n",
        "    return 1.0 / (1.0 + math.exp(-z))\n",
        "\n",
        "def qload(x: float, step: float) -> float:\n",
        "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
        "        return np.nan\n",
        "    return float(np.round(x / step) * step)\n",
        "\n",
        "def qrpe(x: float, step: float) -> float:\n",
        "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
        "        return np.nan\n",
        "    x = float(np.clip(x, 1.0, 10.0))\n",
        "    return float(np.round(x / step) * step)\n",
        "\n",
        "def clamp_int(x, lo, hi):\n",
        "    return int(np.clip(int(round(x)), lo, hi))\n",
        "\n",
        "def sample_split(rng):\n",
        "    return str(rng.choice([\"PPL\", \"FullBody\"], p=[0.7, 0.3]))\n",
        "\n",
        "def exp_weights(L: int, tau: float) -> np.ndarray:\n",
        "    idx = np.arange(L, dtype=float)\n",
        "    return np.exp(-idx / float(tau))\n",
        "\n",
        "def logit(p: float) -> float:\n",
        "    \"\"\"Inverse sigmoid: logit(p) = ln(p/(1-p))\"\"\"\n",
        "    return math.log(p/(1-p))\n"
      ],
      "metadata": {
        "id": "eMlUVblQh_ed"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL 4 — (Python) Load exercise catalog (fallback incluso)**"
      ],
      "metadata": {
        "id": "UWu5yD2HiBVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_exercises_catalog(path: str = \"esercizi_catalogo80.csv\") -> pd.DataFrame:\n",
        "    p = Path(path)\n",
        "    if p.exists():\n",
        "        df = pd.read_csv(p)\n",
        "        # prova a normalizzare nomi “invictus-like”\n",
        "        rename_map = {\n",
        "            \"idEsercizio\": \"exerciseid\",\n",
        "            \"nome\": \"name\",\n",
        "            \"gruppoMuscolare\": \"targetmusclegroup\",\n",
        "            \"livelloEsercizio\": \"difficultylevel\",\n",
        "            \"Mechanics\": \"mechanics\"\n",
        "        }\n",
        "        for k, v in rename_map.items():\n",
        "            if k in df.columns and v not in df.columns:\n",
        "                df = df.rename(columns={k: v})\n",
        "        if \"splitcat\" not in df.columns:\n",
        "            df[\"splitcat\"] = \"other\"\n",
        "        keep = [\"exerciseid\",\"name\",\"targetmusclegroup\",\"mechanics\",\"difficultylevel\",\n",
        "                \"equipment\",\"bodyregion\",\"splitcat\"]\n",
        "        for c in keep:\n",
        "            if c not in df.columns:\n",
        "                df[c] = None\n",
        "        df = df[keep].copy()\n",
        "        df[\"exerciseid\"] = df[\"exerciseid\"].astype(int)\n",
        "        df[\"splitcat\"] = df[\"splitcat\"].astype(str).str.lower()\n",
        "        return df.sort_values(\"exerciseid\").reset_index(drop=True)\n",
        "\n",
        "    # Fallback minimale (estendibile)\n",
        "    rows = [\n",
        "        (1,\"Bench Press\",\"Chest\",\"Compound\",\"Intermediate\",\"Barbell\",\"Upper Body\",\"push\"),\n",
        "        (2,\"Barbell Row\",\"Back\",\"Compound\",\"Intermediate\",\"Barbell\",\"Upper Body\",\"pull\"),\n",
        "        (3,\"Squat\",\"Quadriceps\",\"Compound\",\"Advanced\",\"Barbell\",\"Lower Body\",\"legs\"),\n",
        "        (4,\"Cable Fly\",\"Chest\",\"Isolation\",\"Beginner\",\"Cable\",\"Upper Body\",\"push\"),\n",
        "        (5,\"Lat Pulldown\",\"Back\",\"Compound\",\"Beginner\",\"Machine\",\"Upper Body\",\"pull\"),\n",
        "        (6,\"Leg Press\",\"Quadriceps\",\"Compound\",\"Intermediate\",\"Machine\",\"Lower Body\",\"legs\"),\n",
        "        (7,\"Plank\",\"Abdominals\",\"Compound\",\"Beginner\",\"Bodyweight\",\"Core\",\"core\"),\n",
        "        (8,\"Lateral Raise\",\"Shoulders\",\"Isolation\",\"Beginner\",\"Dumbbell\",\"Upper Body\",\"push\"),\n",
        "        (9,\"Romanian Deadlift\",\"Hamstrings\",\"Compound\",\"Advanced\",\"Barbell\",\"Lower Body\",\"legs\"),\n",
        "        (10,\"Incline DB Press\",\"Chest\",\"Compound\",\"Intermediate\",\"Dumbbell\",\"Upper Body\",\"push\"),\n",
        "        (11,\"Seated Cable Row\",\"Back\",\"Compound\",\"Beginner\",\"Cable\",\"Upper Body\",\"pull\"),\n",
        "        (12,\"Leg Curl\",\"Hamstrings\",\"Isolation\",\"Beginner\",\"Machine\",\"Lower Body\",\"legs\"),\n",
        "    ]\n",
        "    return pd.DataFrame(rows, columns=[\n",
        "        \"exerciseid\",\"name\",\"targetmusclegroup\",\"mechanics\",\"difficultylevel\",\n",
        "        \"equipment\",\"bodyregion\",\"splitcat\"\n",
        "    ])\n",
        "\n",
        "df_ex = load_exercises_catalog()\n",
        "df_ex.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2rmaWBQHiIfS",
        "outputId": "cdaf9590-9856-4215-d1b1-c18fed25d387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   exerciseid          name targetmusclegroup  mechanics difficultylevel  \\\n",
              "0           1   Bench Press             Chest   Compound    Intermediate   \n",
              "1           2   Barbell Row              Back   Compound    Intermediate   \n",
              "2           3         Squat        Quadriceps   Compound        Advanced   \n",
              "3           4     Cable Fly             Chest  Isolation        Beginner   \n",
              "4           5  Lat Pulldown              Back   Compound        Beginner   \n",
              "\n",
              "  equipment  bodyregion splitcat  \n",
              "0   Barbell  Upper Body     push  \n",
              "1   Barbell  Upper Body     pull  \n",
              "2   Barbell  Lower Body     legs  \n",
              "3     Cable  Upper Body     push  \n",
              "4   Machine  Upper Body     pull  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-38695455-edf3-44e3-9578-93addc912cbc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>exerciseid</th>\n",
              "      <th>name</th>\n",
              "      <th>targetmusclegroup</th>\n",
              "      <th>mechanics</th>\n",
              "      <th>difficultylevel</th>\n",
              "      <th>equipment</th>\n",
              "      <th>bodyregion</th>\n",
              "      <th>splitcat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Bench Press</td>\n",
              "      <td>Chest</td>\n",
              "      <td>Compound</td>\n",
              "      <td>Intermediate</td>\n",
              "      <td>Barbell</td>\n",
              "      <td>Upper Body</td>\n",
              "      <td>push</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Barbell Row</td>\n",
              "      <td>Back</td>\n",
              "      <td>Compound</td>\n",
              "      <td>Intermediate</td>\n",
              "      <td>Barbell</td>\n",
              "      <td>Upper Body</td>\n",
              "      <td>pull</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Squat</td>\n",
              "      <td>Quadriceps</td>\n",
              "      <td>Compound</td>\n",
              "      <td>Advanced</td>\n",
              "      <td>Barbell</td>\n",
              "      <td>Lower Body</td>\n",
              "      <td>legs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Cable Fly</td>\n",
              "      <td>Chest</td>\n",
              "      <td>Isolation</td>\n",
              "      <td>Beginner</td>\n",
              "      <td>Cable</td>\n",
              "      <td>Upper Body</td>\n",
              "      <td>push</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Lat Pulldown</td>\n",
              "      <td>Back</td>\n",
              "      <td>Compound</td>\n",
              "      <td>Beginner</td>\n",
              "      <td>Machine</td>\n",
              "      <td>Upper Body</td>\n",
              "      <td>pull</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38695455-edf3-44e3-9578-93addc912cbc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-38695455-edf3-44e3-9578-93addc912cbc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-38695455-edf3-44e3-9578-93addc912cbc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_ex",
              "summary": "{\n  \"name\": \"df_ex\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"exerciseid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 12,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          11,\n          10,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"Seated Cable Row\",\n          \"Incline DB Press\",\n          \"Bench Press\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"targetmusclegroup\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Chest\",\n          \"Back\",\n          \"Hamstrings\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mechanics\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Isolation\",\n          \"Compound\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"difficultylevel\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Intermediate\",\n          \"Advanced\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"equipment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Cable\",\n          \"Dumbbell\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bodyregion\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Upper Body\",\n          \"Lower Body\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"splitcat\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"pull\",\n          \"core\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL 5 — (Python) Sample user latents + per-user date windows**"
      ],
      "metadata": {
        "id": "8lDBDsfUiKhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEVELS = [\"Beginner\", \"Intermediate\", \"Advanced\"]\n",
        "\n",
        "def sample_user_window(cfg: CFG, rng) -> tuple[date, date]:\n",
        "    end_date = cfg.today + timedelta(days=int(rng.integers(0, cfg.end_date_max_days_ahead + 1)))\n",
        "    dur = int(rng.integers(cfg.min_duration_days, cfg.max_duration_days + 1))\n",
        "    start_date = end_date - timedelta(days=dur)\n",
        "    return start_date, end_date\n",
        "\n",
        "def sample_user_latents(cfg: CFG, rng):\n",
        "    # experience_latent in [0,1], 0=novice-ish, 1=advanced-ish\n",
        "    exp_lat = float(np.clip(rng.beta(2.0, 2.0), 0.0, 1.0))\n",
        "\n",
        "    # Parametri continui (niente \"if experience then ...\")\n",
        "    # adattamento: più alto per exp_lat bassa\n",
        "    alpha = float(np.clip(rng.normal(0.05 - 0.03*exp_lat, 0.01), 0.005, 0.08))\n",
        "    # detraining: più alto per exp_lat bassa\n",
        "    k_d = float(np.clip(rng.normal(0.020 - 0.012*exp_lat, 0.004), 0.002, 0.03))\n",
        "    # noise: più alto per exp_lat bassa\n",
        "    obs_noise = float(np.clip(rng.normal(0.25 - 0.18*exp_lat, 0.05), 0.03, 0.35))\n",
        "\n",
        "    resilience = float(np.clip(rng.normal(1.0 + 0.6*exp_lat, 0.25), 0.4, 2.2))\n",
        "    fatigue_sens = float(np.clip(rng.lognormal(mean=-0.2, sigma=0.35), 0.2, 2.0))\n",
        "\n",
        "    return dict(\n",
        "        experience_latent=exp_lat,\n",
        "        alpha_adapt=alpha,\n",
        "        k_detraining=k_d,\n",
        "        obs_noise=obs_noise,\n",
        "        resilience=resilience,\n",
        "        fatigue_sens=fatigue_sens,\n",
        "        rpe_report_bias=float(rng.normal(0.0, 0.35)),\n",
        "    )\n",
        "\n",
        "def latents_to_experience_label(exp_lat: float) -> str:\n",
        "    # discretizzazione semplice (puoi cambiarla in quantili globali)\n",
        "    # 3 classi: Beginner / Intermediate / Advanced\n",
        "    if exp_lat < 0.40:\n",
        "        return \"Beginner\"\n",
        "    if exp_lat < 0.80:\n",
        "        return \"Intermediate\"\n",
        "    return \"Advanced\"\n",
        "\n",
        "def generate_users(cfg: CFG, rng) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for uid in range(1, cfg.n_users + 1):\n",
        "        start_u, end_u = sample_user_window(cfg, rng)\n",
        "        weekly_freq = clamp_int(rng.normal(cfg.weekly_freq_mu, cfg.weekly_freq_sd),\n",
        "                                cfg.weekly_freq_min, cfg.weekly_freq_max)\n",
        "        split = sample_split(rng)\n",
        "\n",
        "        lat = sample_user_latents(cfg, rng)\n",
        "        exp_label = latents_to_experience_label(lat[\"experience_latent\"])\n",
        "\n",
        "        rows.append({\n",
        "            \"userid\": uid,\n",
        "            \"weeklyfreqdeclared\": weekly_freq,\n",
        "            \"splittype\": split,\n",
        "            \"start_date\": start_u.isoformat(),\n",
        "            \"end_date\": end_u.isoformat(),\n",
        "\n",
        "            # label target (non nei log)\n",
        "            \"experience_label\": exp_label,\n",
        "\n",
        "            # latenti (debug: puoi anche NON esportarli se vuoi)\n",
        "            \"experience_latent\": round(lat[\"experience_latent\"], 4),\n",
        "            \"alpha_adapt\": round(lat[\"alpha_adapt\"], 5),\n",
        "            \"k_detraining\": round(lat[\"k_detraining\"], 5),\n",
        "            \"obs_noise\": round(lat[\"obs_noise\"], 4),\n",
        "            \"resilience\": round(lat[\"resilience\"], 4),\n",
        "            \"fatigue_sens\": round(lat[\"fatigue_sens\"], 4),\n",
        "            \"rpe_report_bias\": round(lat[\"rpe_report_bias\"], 4),\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "df_users = generate_users(cfg, rng)\n",
        "df_users.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ILGOMkqLiOB2",
        "outputId": "765e7f00-8c80-4659-b19f-fe125b481f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   userid  weeklyfreqdeclared splittype  start_date    end_date  \\\n",
              "0       1                   2  FullBody  2024-09-12  2026-04-03   \n",
              "1       2                   4       PPL  2025-03-25  2026-11-18   \n",
              "2       3                   4       PPL  2025-11-07  2026-10-21   \n",
              "3       4                   4  FullBody  2025-11-07  2027-03-16   \n",
              "4       5                   4  FullBody  2026-02-01  2027-09-02   \n",
              "\n",
              "  experience_label  experience_latent  alpha_adapt  k_detraining  obs_noise  \\\n",
              "0         Advanced             0.8678      0.02080       0.00952     0.0511   \n",
              "1     Intermediate             0.4157      0.03568       0.01229     0.2363   \n",
              "2     Intermediate             0.6568      0.02216       0.01458     0.1882   \n",
              "3         Beginner             0.3741      0.03994       0.01639     0.2262   \n",
              "4     Intermediate             0.5701      0.04785       0.00970     0.1958   \n",
              "\n",
              "   resilience  fatigue_sens  rpe_report_bias  \n",
              "0      1.7405        1.0749           0.0231  \n",
              "1      1.2108        0.7047          -0.1232  \n",
              "2      1.3656        0.6101          -0.2886  \n",
              "3      1.2804        1.0383           0.0237  \n",
              "4      0.9214        0.7282           0.0570  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f84b825f-902a-4576-88be-942310992fa0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userid</th>\n",
              "      <th>weeklyfreqdeclared</th>\n",
              "      <th>splittype</th>\n",
              "      <th>start_date</th>\n",
              "      <th>end_date</th>\n",
              "      <th>experience_label</th>\n",
              "      <th>experience_latent</th>\n",
              "      <th>alpha_adapt</th>\n",
              "      <th>k_detraining</th>\n",
              "      <th>obs_noise</th>\n",
              "      <th>resilience</th>\n",
              "      <th>fatigue_sens</th>\n",
              "      <th>rpe_report_bias</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>FullBody</td>\n",
              "      <td>2024-09-12</td>\n",
              "      <td>2026-04-03</td>\n",
              "      <td>Advanced</td>\n",
              "      <td>0.8678</td>\n",
              "      <td>0.02080</td>\n",
              "      <td>0.00952</td>\n",
              "      <td>0.0511</td>\n",
              "      <td>1.7405</td>\n",
              "      <td>1.0749</td>\n",
              "      <td>0.0231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>PPL</td>\n",
              "      <td>2025-03-25</td>\n",
              "      <td>2026-11-18</td>\n",
              "      <td>Intermediate</td>\n",
              "      <td>0.4157</td>\n",
              "      <td>0.03568</td>\n",
              "      <td>0.01229</td>\n",
              "      <td>0.2363</td>\n",
              "      <td>1.2108</td>\n",
              "      <td>0.7047</td>\n",
              "      <td>-0.1232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>PPL</td>\n",
              "      <td>2025-11-07</td>\n",
              "      <td>2026-10-21</td>\n",
              "      <td>Intermediate</td>\n",
              "      <td>0.6568</td>\n",
              "      <td>0.02216</td>\n",
              "      <td>0.01458</td>\n",
              "      <td>0.1882</td>\n",
              "      <td>1.3656</td>\n",
              "      <td>0.6101</td>\n",
              "      <td>-0.2886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>FullBody</td>\n",
              "      <td>2025-11-07</td>\n",
              "      <td>2027-03-16</td>\n",
              "      <td>Beginner</td>\n",
              "      <td>0.3741</td>\n",
              "      <td>0.03994</td>\n",
              "      <td>0.01639</td>\n",
              "      <td>0.2262</td>\n",
              "      <td>1.2804</td>\n",
              "      <td>1.0383</td>\n",
              "      <td>0.0237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>FullBody</td>\n",
              "      <td>2026-02-01</td>\n",
              "      <td>2027-09-02</td>\n",
              "      <td>Intermediate</td>\n",
              "      <td>0.5701</td>\n",
              "      <td>0.04785</td>\n",
              "      <td>0.00970</td>\n",
              "      <td>0.1958</td>\n",
              "      <td>0.9214</td>\n",
              "      <td>0.7282</td>\n",
              "      <td>0.0570</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f84b825f-902a-4576-88be-942310992fa0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f84b825f-902a-4576-88be-942310992fa0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f84b825f-902a-4576-88be-942310992fa0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_users",
              "summary": "{\n  \"name\": \"df_users\",\n  \"rows\": 300,\n  \"fields\": [\n    {\n      \"column\": \"userid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 86,\n        \"min\": 1,\n        \"max\": 300,\n        \"num_unique_values\": 300,\n        \"samples\": [\n          204,\n          267,\n          153\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"weeklyfreqdeclared\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 6,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          2,\n          4,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"splittype\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"PPL\",\n          \"FullBody\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start_date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 261,\n        \"samples\": [\n          \"2026-06-01\",\n          \"2026-06-05\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end_date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 252,\n        \"samples\": [\n          \"2027-10-22\",\n          \"2026-04-30\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"experience_label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Advanced\",\n          \"Intermediate\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"experience_latent\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.21688288810903228,\n        \"min\": 0.0028,\n        \"max\": 0.9735,\n        \"num_unique_values\": 297,\n        \"samples\": [\n          0.6126,\n          0.4947\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"alpha_adapt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.011494054515832604,\n        \"min\": 0.005,\n        \"max\": 0.06545,\n        \"num_unique_values\": 291,\n        \"samples\": [\n          0.05047,\n          0.0307\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"k_detraining\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004659758015329441,\n        \"min\": 0.002,\n        \"max\": 0.02617,\n        \"num_unique_values\": 272,\n        \"samples\": [\n          0.0158,\n          0.01799\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"obs_noise\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06478449920838443,\n        \"min\": 0.03,\n        \"max\": 0.3386,\n        \"num_unique_values\": 270,\n        \"samples\": [\n          0.1518,\n          0.2181\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"resilience\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2715539004251648,\n        \"min\": 0.5809,\n        \"max\": 2.0989,\n        \"num_unique_values\": 296,\n        \"samples\": [\n          1.0964,\n          1.2386\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fatigue_sens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.30599400339702526,\n        \"min\": 0.3178,\n        \"max\": 2.0,\n        \"num_unique_values\": 293,\n        \"samples\": [\n          0.7432,\n          1.2058\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rpe_report_bias\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3833739279890606,\n        \"min\": -1.0257,\n        \"max\": 1.081,\n        \"num_unique_values\": 299,\n        \"samples\": [\n          0.0033,\n          -0.0797\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL 6 — (Python) Capabilities (cmax) + templates per split**"
      ],
      "metadata": {
        "id": "hsFmRqq5i6aD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASEMAP = {\"Beginner\": 50.0, \"Intermediate\": 80.0, \"Advanced\": 105.0}\n",
        "\n",
        "def build_capabilities(df_users: pd.DataFrame, df_ex: pd.DataFrame, rng) -> dict:\n",
        "    caps = {}\n",
        "    for u in df_users.itertuples(index=False):\n",
        "        uid = int(u.userid)\n",
        "        exp_label = str(u.experience_label)\n",
        "        exp_lat = float(u.experience_latent)\n",
        "\n",
        "        # scala base da label (solo per cmax \"medio\"), ma con jitter continuo su exp_lat\n",
        "        # Nota: non stiamo usando la label per governare dinamiche; è solo un prior sul massimo teorico.\n",
        "        base_factor = BASEMAP.get(exp_label, 70.0) * (0.85 + 0.30*exp_lat)\n",
        "\n",
        "        usermap = {}\n",
        "        for ex in df_ex.itertuples(index=False):\n",
        "            # difficoltà esercizio influenza cmax relativo\n",
        "            diff = str(ex.difficultylevel)\n",
        "            diff_mul = {\"Beginner\": 0.85, \"Intermediate\": 1.0, \"Advanced\": 1.12}.get(diff, 1.0)\n",
        "            cmax = base_factor * diff_mul * float(rng.normal(1.0, 0.12))\n",
        "            cmax = float(np.clip(cmax, 10.0, 200.0))\n",
        "            usermap[int(ex.exerciseid)] = qload(cmax, cfg.load_step)\n",
        "        caps[uid] = usermap\n",
        "    return caps\n",
        "\n",
        "PPL_ROT = [\"Push\", \"Pull\", \"Legs\"]\n",
        "FB_ROT = [\"FullBody-A\", \"FullBody-B\", \"FullBody-C\"]\n",
        "\n",
        "def choose_exercises_for_tag(df_ex: pd.DataFrame, tag: str, rng, n_min=3, n_max=6):\n",
        "    if tag.startswith(\"FullBody\"):\n",
        "        pools = {\n",
        "            \"legs\": df_ex[df_ex[\"splitcat\"].isin([\"legs\"])],\n",
        "            \"push\": df_ex[df_ex[\"splitcat\"].isin([\"push\"])],\n",
        "            \"pull\": df_ex[df_ex[\"splitcat\"].isin([\"pull\"])],\n",
        "            \"core\": df_ex[df_ex[\"splitcat\"].isin([\"core\"])],\n",
        "            \"other\": df_ex[~df_ex[\"splitcat\"].isin([\"legs\",\"push\",\"pull\",\"core\"])],\n",
        "        }\n",
        "        exids = []\n",
        "        for k, n in [(\"legs\",1),(\"push\",1),(\"pull\",1)]:\n",
        "            if len(pools[k]) > 0:\n",
        "                exids += rng.choice(pools[k][\"exerciseid\"].values, size=n, replace=False).tolist()\n",
        "        if len(pools[\"core\"]) and rng.random() < 0.6:\n",
        "            exids += rng.choice(pools[\"core\"][\"exerciseid\"].values, size=1, replace=False).tolist()\n",
        "        while len(exids) < 4:\n",
        "            pool = pools[\"other\"] if len(pools[\"other\"]) else df_ex\n",
        "            exids += rng.choice(pool[\"exerciseid\"].values, size=1, replace=False).tolist()\n",
        "        # dedup mantenendo ordine\n",
        "        seen = set()\n",
        "        exids2 = []\n",
        "        for x in exids:\n",
        "            if x not in seen:\n",
        "                seen.add(x)\n",
        "                exids2.append(int(x))\n",
        "        return exids2[:6]\n",
        "\n",
        "    # PPL\n",
        "    tag_l = tag.lower()\n",
        "    pool = df_ex[df_ex[\"splitcat\"] == tag_l]\n",
        "    if len(pool) < 3:\n",
        "        pool = df_ex.copy()\n",
        "    n = int(rng.integers(n_min, n_max + 1))\n",
        "    n = min(n, len(pool))\n",
        "    return [int(x) for x in rng.choice(pool[\"exerciseid\"].values, size=n, replace=False)]\n",
        "\n",
        "def build_user_templates(df_users, df_ex, rng):\n",
        "    templates = {}\n",
        "    for u in df_users.itertuples(index=False):\n",
        "        uid = int(u.userid)\n",
        "        split = str(u.splittype)\n",
        "        tags = PPL_ROT if split == \"PPL\" else FB_ROT\n",
        "        templates[uid] = {tag: choose_exercises_for_tag(df_ex, tag, rng) for tag in tags}\n",
        "    return templates\n",
        "\n",
        "caps = build_capabilities(df_users, df_ex, rng)\n",
        "templates = build_user_templates(df_users, df_ex, rng)\n",
        "\n",
        "list(templates.items())[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikWIqn-aiPty",
        "outputId": "a4da0b9e-8b52-49fa-e488-a3af6f05191b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " {'FullBody-A': [6, 4, 5, 12],\n",
              "  'FullBody-B': [9, 10, 5, 12],\n",
              "  'FullBody-C': [9, 4, 5, 7]})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL 7 — (Python) Plan prescription (per esercizio) + helpers intensità**"
      ],
      "metadata": {
        "id": "XQf8GEsui94U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prescribe_exercise(df_ex_row: dict, experience_label: str, rng):\n",
        "    # Range reps/sets/rest/rir per esercizio (semplice e stabile)\n",
        "    lvl = str(df_ex_row[\"difficultylevel\"])\n",
        "\n",
        "    # default base su difficoltà esercizio (non su esperienza!)\n",
        "    if lvl == \"Beginner\":\n",
        "        reps_mu, reps_sd = 12, 2\n",
        "        sets_lo, sets_hi = 2, 4\n",
        "        rest_lo, rest_hi = 60, 120\n",
        "    elif lvl == \"Intermediate\":\n",
        "        reps_mu, reps_sd = 8, 2\n",
        "        sets_lo, sets_hi = 3, 5\n",
        "        rest_lo, rest_hi = 90, 180\n",
        "    else:  # Advanced/altro\n",
        "        reps_mu, reps_sd = 6, 2\n",
        "        sets_lo, sets_hi = 3, 6\n",
        "        rest_lo, rest_hi = 120, 240\n",
        "\n",
        "    # RIR target può dipendere dall'esperienza (scelta coaching), ma non entra come \"flag di dinamica\"\n",
        "    # (serve per definire la prescrizione, che è osservabile nel plan)\n",
        "    if experience_label in [\"Beginner\", \"Novice\"]:\n",
        "        rir_mu = 2.5\n",
        "    elif experience_label == \"Intermediate\":\n",
        "        rir_mu = 2.0\n",
        "    else:\n",
        "        rir_mu = 1.5\n",
        "\n",
        "    setsplanned = int(rng.integers(sets_lo, sets_hi + 1))\n",
        "    repsmid = clamp_int(rng.normal(reps_mu, reps_sd), 3, 20)\n",
        "    width = 2 if lvl != \"Beginner\" else 3\n",
        "    repsmin = max(1, repsmid - width)\n",
        "    repsmax = min(30, repsmid + width)\n",
        "\n",
        "    restplannedsec = int(rng.integers(rest_lo, rest_hi + 1))\n",
        "    rirtarget = clamp_int(rng.normal(rir_mu, 0.6), 0, 5)\n",
        "\n",
        "    return setsplanned, repsmin, repsmax, restplannedsec, rirtarget\n",
        "\n",
        "def intensity_from_reps_rir(reps_target: float, rir_target: float, rng):\n",
        "    # euristica: più reps e più RIR => intensità minore\n",
        "    base = 0.86 - 0.018*(reps_target - 5.0) - 0.03*rir_target\n",
        "    base += float(rng.normal(0.0, 0.02))\n",
        "    return float(np.clip(base, 0.35, 0.92))\n"
      ],
      "metadata": {
        "id": "7xU2bkt4jBj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL 8 — (Python) Scheduler per utente + simulazione set-level (core)**"
      ],
      "metadata": {
        "id": "f25VNy0ajC14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def schedule_sessions_for_user(start_u: date, end_u: date, weekly_freq: int, rng):\n",
        "    # giorni target della settimana\n",
        "    basedays = sorted(rng.choice(np.arange(7), size=weekly_freq, replace=False).tolist())\n",
        "    dates = []\n",
        "    d0 = start_u\n",
        "    n_days = (end_u - start_u).days + 1\n",
        "    for i in range(n_days):\n",
        "        day = d0 + timedelta(days=i)\n",
        "        if day.weekday() in basedays:\n",
        "            # jitter -1/0/+1\n",
        "            jitter = int(rng.choice([-1,0,1], p=cfg.weekday_jitter_probs))\n",
        "            day2 = day + timedelta(days=jitter)\n",
        "            if start_u <= day2 <= end_u:\n",
        "                dates.append(day2)\n",
        "    dates = sorted(list(set(dates)))\n",
        "    return dates\n",
        "\n",
        "def simulate_user(cfg: CFG, user_row: dict, df_ex: pd.DataFrame, caps_u: dict, templates_u: dict, rng):\n",
        "    uid = int(user_row[\"userid\"])\n",
        "    start_u = date.fromisoformat(user_row[\"start_date\"])\n",
        "    end_u = date.fromisoformat(user_row[\"end_date\"])\n",
        "    weekly_freq = int(user_row[\"weeklyfreqdeclared\"])\n",
        "    experience_label = str(user_row[\"experience_label\"])\n",
        "\n",
        "    # latenti (stato + parametri)\n",
        "    exp_lat = float(user_row[\"experience_latent\"])\n",
        "    alpha = float(user_row[\"alpha_adapt\"])\n",
        "    k_d = float(user_row[\"k_detraining\"])\n",
        "    obs_noise = float(user_row[\"obs_noise\"])\n",
        "    resilience = float(user_row[\"resilience\"])\n",
        "    fatigue_sens = float(user_row[\"fatigue_sens\"])\n",
        "    rpe_bias = float(user_row[\"rpe_report_bias\"])\n",
        "\n",
        "    # Banister params per utente\n",
        "    tauF = float(max(7.0, rng.normal(cfg.tauF_mean, cfg.tauF_sd)))\n",
        "    tauD = float(max(2.0, rng.normal(cfg.tauD_mean, cfg.tauD_sd)))\n",
        "\n",
        "    # stato dinamico (scalari)\n",
        "    fitness = float(rng.normal(0.0, 1.0))\n",
        "    fatigue = float(max(0.0, rng.normal(0.5, 0.3)))\n",
        "    skill = float(np.clip(rng.normal(0.2 + 0.6*exp_lat, 0.15), 0.0, 2.0))\n",
        "\n",
        "    # injury state\n",
        "    injury_until = None\n",
        "\n",
        "    # schedule “candidato”\n",
        "    session_dates = schedule_sessions_for_user(start_u, end_u, weekly_freq, rng)\n",
        "\n",
        "    workouts_rows = []\n",
        "    plan_rows = []\n",
        "    sets_rows = []\n",
        "    impulse_rows = []\n",
        "\n",
        "    wid = 1  # per-user counter, poi lo rendiamo globale fuori\n",
        "    set_id_counter = 1\n",
        "\n",
        "    # rotazione tag per split\n",
        "    tags = PPL_ROT if str(user_row[\"splittype\"]) == \"PPL\" else FB_ROT\n",
        "    tag_i = int(rng.integers(0, len(tags)))\n",
        "\n",
        "    last_train_date = None\n",
        "\n",
        "    for d in session_dates:\n",
        "        # detraining: se gap\n",
        "        if last_train_date is not None:\n",
        "            gap = (d - last_train_date).days\n",
        "            if gap > 1:\n",
        "                fitness *= math.exp(-k_d * gap)\n",
        "\n",
        "        # decay fatica giornaliero\n",
        "        fatigue *= math.exp(-1.0/7.0)\n",
        "\n",
        "        # se infortunio\n",
        "        in_injury = (injury_until is not None and d <= injury_until)\n",
        "\n",
        "        # skip probability\n",
        "        # --- SKIP MODEL (baseline per livello + fatica leggera) ---\n",
        "        p0 = float(cfg.skip_p0_by_level.get(experience_label, 0.10))\n",
        "        bias = logit(p0)\n",
        "\n",
        "        fat_term = float(np.log1p(max(0.0, float(fatigue))))\n",
        "        fat_term = min(fat_term, float(cfg.skip_fatigue_cap))\n",
        "\n",
        "        z = bias + float(cfg.skip_fatigue_weight) * fat_term + float(rng.normal(0.0, cfg.skip_noise_sd))\n",
        "        p_skip = sigmoid(z)\n",
        "\n",
        "        status = \"done\"\n",
        "        if rng.random() < p_skip:\n",
        "            status = \"skipped\"\n",
        "        # ----------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "        # assegna tag sessione\n",
        "        tag = tags[tag_i % len(tags)]\n",
        "        tag_i += 1\n",
        "\n",
        "        week_index_user = (d - start_u).days // 7 + 1\n",
        "\n",
        "        workouts_rows.append({\n",
        "            \"userid\": uid,\n",
        "            \"date\": d.isoformat(),\n",
        "            \"weekindex_user\": int(week_index_user),\n",
        "            \"sessiontag\": tag,\n",
        "            \"workoutstatus\": status,\n",
        "            \"z_skip\": float(z),\n",
        "            \"p_skip\": float(p_skip),\n",
        "            \"fatigue_term\": float(fat_term),\n",
        "            \"fatigue_mult\": float(fatigue_mult),\n",
        "            \"experience_label\": experience_label,\n",
        "        })\n",
        "\n",
        "        if status == \"skipped\":\n",
        "            impulse_rows.append({\"userid\": uid, \"date\": d.isoformat(), \"impulse\": 0.0})\n",
        "            continue\n",
        "\n",
        "        # plan per esercizio (per questa sessione)\n",
        "        exids = templates_u.get(tag, [])\n",
        "        if len(exids) == 0:\n",
        "            exids = templates_u[list(templates_u.keys())[0]]\n",
        "\n",
        "        # fatica intra-sessione (scalare semplice)\n",
        "        fatigue_session = float(fatigue)\n",
        "\n",
        "        day_impulse = 0.0\n",
        "        day_total_sets = 0\n",
        "\n",
        "        for exid in exids:\n",
        "            exrow = df_ex[df_ex[\"exerciseid\"] == exid].iloc[0].to_dict()\n",
        "            setsplanned, repsmin, repsmax, restplannedsec, rirtarget = prescribe_exercise(exrow, experience_label, rng)\n",
        "\n",
        "            # eventuale riduzione volume in injury\n",
        "            if in_injury:\n",
        "                setsplanned = max(1, int(round(setsplanned * 0.6)))\n",
        "\n",
        "            plan_rows.append({\n",
        "                \"userid\": uid,\n",
        "                \"date\": d.isoformat(),\n",
        "                \"sessiontag\": tag,\n",
        "                \"exerciseid\": int(exid),\n",
        "                \"setsplanned\": int(setsplanned),\n",
        "                \"repsmin\": int(repsmin),\n",
        "                \"repsmax\": int(repsmax),\n",
        "                \"restplannedsec\": int(restplannedsec),\n",
        "                \"rirtarget\": int(rirtarget),\n",
        "            })\n",
        "\n",
        "            # simula set per set\n",
        "            cmax = float(caps_u[int(exid)])\n",
        "\n",
        "            # intended baseline load (per esercizio) dalla prima serie\n",
        "            reps_target0 = int(rng.integers(repsmin, repsmax + 1))\n",
        "            inten0 = intensity_from_reps_rir(reps_target0, rirtarget, rng)\n",
        "            intended_load = qload(inten0 * cmax, cfg.load_step)\n",
        "\n",
        "            # “esecuzione”: setdone ~ setsplanned con rumore/aderenza implicita\n",
        "            setsdone = int(np.clip(round(rng.normal(setsplanned, 0.5)), 1, 10))\n",
        "\n",
        "            for s in range(1, setsdone + 1):\n",
        "                day_total_sets += 1\n",
        "\n",
        "                reps_target = int(rng.integers(repsmin, repsmax + 1))\n",
        "                inten = intensity_from_reps_rir(reps_target, rirtarget, rng)\n",
        "\n",
        "                # fatica riduce la capacità \"real-time\"\n",
        "                fatigue_factor = float(np.clip(0.06 * fatigue_session * fatigue_sens, 0.0, 0.35))\n",
        "                load_done = float(inten * cmax * (1.0 - fatigue_factor))\n",
        "                load_done *= float(rng.normal(1.0, 0.03 + obs_noise*0.08))\n",
        "                load_done = qload(float(np.clip(load_done, 2.5, cmax)), cfg.load_step)\n",
        "\n",
        "                # reps calano se fatica sale\n",
        "                reps_done = int(np.clip(round(rng.normal(reps_target * (1.0 - 0.20*fatigue_factor), 0.6 + obs_noise*1.5)),\n",
        "                                        1, 30))\n",
        "\n",
        "                # RPE cresce con intensità e fatica e gap reps\n",
        "                rep_gap = (reps_target - reps_done) / max(1.0, reps_target)\n",
        "                rpe_true = 4.0 + 5.5*inten + 1.8*rep_gap + 1.2*fatigue_factor\n",
        "                rpe_obs = float(rng.normal(rpe_true + rpe_bias, 0.35 + obs_noise))\n",
        "                rpe_done = qrpe(rpe_obs, cfg.rpe_step)\n",
        "\n",
        "                # feedback raro\n",
        "                feedback = None\n",
        "                if rng.random() < 0.03:\n",
        "                    feedback = str(rng.choice([\n",
        "                        \"Tecnica ok\", \"Fatica alta\", \"Allenamento solido\", \"Recuperi corti\", \"Non ero in giornata\"\n",
        "                    ]))\n",
        "\n",
        "                # missingness (solo osservazioni)\n",
        "                if rng.random() < cfg.p_missing_rpe:\n",
        "                    rpe_done = np.nan\n",
        "                if rng.random() < cfg.p_missing_load:\n",
        "                    load_done = np.nan\n",
        "                if rng.random() < cfg.p_missing_feedback:\n",
        "                    feedback = None\n",
        "\n",
        "                sets_rows.append({\n",
        "                    \"set_id\": f\"U{uid:04d}_S{set_id_counter:07d}\",\n",
        "                    \"userid\": uid,\n",
        "                    \"date\": d.isoformat(),\n",
        "                    \"weekindex_user\": int(week_index_user),\n",
        "                    \"sessiontag\": tag,\n",
        "                    \"exerciseid\": int(exid),\n",
        "\n",
        "                    \"set_index\": int(s),\n",
        "\n",
        "                    \"reps_target\": int(reps_target),\n",
        "                    \"reps_done\": int(reps_done),\n",
        "                    \"load_intended_kg\": float(intended_load),\n",
        "                    \"load_done_kg\": load_done,\n",
        "                    \"rpe_done\": rpe_done,\n",
        "\n",
        "                    \"restplannedsec\": int(restplannedsec),\n",
        "                    \"rirtarget\": int(rirtarget),\n",
        "                    \"feedback\": feedback,\n",
        "                })\n",
        "                set_id_counter += 1\n",
        "\n",
        "                # impulso giornaliero (Banister input)\n",
        "                ld = 0.0 if (isinstance(load_done, float) and np.isnan(load_done)) else float(load_done)\n",
        "                rd = 0.0 if (isinstance(rpe_done, float) and np.isnan(rpe_done)) else float(rpe_done)\n",
        "                day_impulse += ld * float(reps_done) * (rd / 10.0)\n",
        "\n",
        "                # aggiorna fatica intra-sessione\n",
        "                fatigue_session += 0.08 * inten + 0.02 * (ld / max(20.0, cmax))\n",
        "\n",
        "            # aggiorna fitness/fatica/skill post-esercizio (molto semplice)\n",
        "            # carico “effettivo” = intended * volume relativo\n",
        "            vol_proxy = setsdone * reps_target0 * float(intended_load)\n",
        "            fitness += alpha * math.log1p(vol_proxy / 1000.0)\n",
        "            skill += 0.002 * math.log1p(1.0 + exp_lat)  # crescita lenta\n",
        "\n",
        "        # injury event (dopo sessione): aumenta con fatica e “impulso” e bassa resilienza\n",
        "        p_injury = cfg.injury_lambda * (day_impulse / max(1.0, resilience)) * (1.0 + 0.5*fadigue_sens if (fadigue_sens:=fatigue_sens) else 1.0)\n",
        "        p_injury = float(np.clip(p_injury, 0.0, 0.35))\n",
        "        if (injury_until is None or d > injury_until) and rng.random() < p_injury:\n",
        "            injury_days = int(rng.integers(cfg.injury_days_min, cfg.injury_days_max + 1))\n",
        "            injury_until = d + timedelta(days=injury_days)\n",
        "\n",
        "        # aggiorna fatica globale a fine sessione\n",
        "        fatigue = float(np.clip(fatigue_session, 0.0, 20.0))\n",
        "\n",
        "        impulse_rows.append({\"userid\": uid, \"date\": d.isoformat(), \"impulse\": float(day_impulse)})\n",
        "        last_train_date = d\n",
        "\n",
        "    # output user-level metadata Banister\n",
        "    user_meta = {\n",
        "        \"userid\": uid,\n",
        "        \"tauF\": tauF,\n",
        "        \"tauD\": tauD,\n",
        "        \"betaF\": cfg.betaF,\n",
        "        \"betaD\": cfg.betaD\n",
        "    }\n",
        "\n",
        "    return workouts_rows, plan_rows, sets_rows, impulse_rows, user_meta\n"
      ],
      "metadata": {
        "id": "nNQf1dvljGCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONTROLLO SKIP**"
      ],
      "metadata": {
        "id": "QpKk8MbJWmgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = OUTDIR if \"OUTDIR\" in globals() else Path(\"data_synth_setlevel\")\n",
        "\n",
        "users = pd.read_csv(DATA_DIR / \"users.csv\")\n",
        "workouts = pd.read_csv(DATA_DIR / \"workouts.csv\")\n",
        "\n",
        "# trova automaticamente la colonna label in users\n",
        "label_col = \"experience_label\" if \"experience_label\" in users.columns else (\n",
        "    \"experiencedeclared\" if \"experiencedeclared\" in users.columns else None\n",
        ")\n",
        "if label_col is None:\n",
        "    raise ValueError(f\"Nessuna colonna esperienza in users.csv. Colonne: {list(users.columns)}\")\n",
        "\n",
        "# porta la label dentro workouts via merge su userid\n",
        "work = workouts.merge(users[[\"userid\", label_col]], on=\"userid\", how=\"left\")  # merge su userid [web:153]\n",
        "\n",
        "work[\"is_skipped\"] = (work[\"workoutstatus\"] == \"skipped\").astype(int)\n",
        "\n",
        "print(\"Skip-rate per livello:\")\n",
        "print(work.groupby(label_col)[\"is_skipped\"].mean().sort_index())  # groupby + mean [web:157]\n",
        "\n",
        "# debug opzionale: se hai z_skip/p_skip loggati\n",
        "for c in [\"z_skip\", \"p_skip\", \"fatigue_term\", \"fatigue_mult\"]:\n",
        "    if c in work.columns:\n",
        "        print(f\"\\nMean {c} per livello:\")\n",
        "        print(work.groupby(label_col)[c].mean().sort_index())\n"
      ],
      "metadata": {
        "id": "8TOpSIfDWpBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = OUTDIR if \"OUTDIR\" in globals() else Path(\"data_synth_setlevel\")\n",
        "workouts = pd.read_csv(DATA_DIR / \"workouts.csv\")\n",
        "\n",
        "print(\"Columns:\", list(workouts.columns))\n",
        "print(workouts[[\"workoutstatus\"]].value_counts().head(10))\n",
        "\n",
        "# Se NON vedi z_skip/p_skip qui, stai guardando un workouts.csv generato da un'altra cella\n",
        "need = {\"z_skip\",\"p_skip\",\"fatigue_term\",\"fatigue_mult\"}\n",
        "print(\"Has skip debug cols:\", need.issubset(set(workouts.columns)))\n",
        "\n",
        "if need.issubset(set(workouts.columns)):\n",
        "    print(workouts[[\"z_skip\",\"p_skip\",\"fatigue_term\",\"fatigue_mult\"]].describe().round(4))\n",
        "    print(\"Mean p_skip:\", workouts[\"p_skip\"].mean())\n",
        "    print(\"Mean is_skipped:\", (workouts[\"workoutstatus\"]==\"skipped\").mean())\n"
      ],
      "metadata": {
        "id": "5NKbBrEbdGIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL 9 — (Python) Run generator (tutti utenti) + IDs globali**"
      ],
      "metadata": {
        "id": "KS6axoF_jHnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_workouts = []\n",
        "all_plan = []\n",
        "all_sets = []\n",
        "all_impulse = []\n",
        "ban_meta_rows = []\n",
        "\n",
        "workout_id_counter = 1\n",
        "\n",
        "for u in df_users.to_dict(orient=\"records\"):\n",
        "    uid = int(u[\"userid\"])\n",
        "    w_rows, p_rows, s_rows, i_rows, meta = simulate_user(cfg, u, df_ex, caps[uid], templates[uid], rng)\n",
        "    ban_meta_rows.append(meta)\n",
        "\n",
        "    # assegna workout_id globale: stesso id per stessa (userid,date)\n",
        "    # costruisco mapping per user\n",
        "    wdf = pd.DataFrame(w_rows)\n",
        "    if len(wdf) == 0:\n",
        "        continue\n",
        "\n",
        "    # sort e assegnazione\n",
        "    wdf = wdf.sort_values([\"userid\",\"date\"]).reset_index(drop=True)\n",
        "    wdf[\"workoutid\"] = np.arange(workout_id_counter, workout_id_counter + len(wdf))\n",
        "    workout_id_counter += len(wdf)\n",
        "\n",
        "    # mapping (userid,date) -> workoutid\n",
        "    key_to_wid = {(int(r.userid), str(r.date)): int(r.workoutid) for r in wdf.itertuples(index=False)}\n",
        "\n",
        "    # push workouts\n",
        "    all_workouts.append(wdf)\n",
        "\n",
        "    # attach workoutid to plan/sets\n",
        "    pdf = pd.DataFrame(p_rows)\n",
        "    if len(pdf):\n",
        "        pdf[\"workoutid\"] = [key_to_wid[(int(r[\"userid\"]), str(r[\"date\"]))] for r in pdf.to_dict(\"records\")]\n",
        "        all_plan.append(pdf)\n",
        "\n",
        "    sdf = pd.DataFrame(s_rows)\n",
        "    if len(sdf):\n",
        "        sdf[\"workoutid\"] = [key_to_wid[(int(r[\"userid\"]), str(r[\"date\"]))] for r in sdf.to_dict(\"records\")]\n",
        "        all_sets.append(sdf)\n",
        "\n",
        "    idf = pd.DataFrame(i_rows)\n",
        "    if len(idf):\n",
        "        all_impulse.append(idf)\n",
        "\n",
        "df_workouts = pd.concat(all_workouts, ignore_index=True) if all_workouts else pd.DataFrame()\n",
        "df_plan = pd.concat(all_plan, ignore_index=True) if all_plan else pd.DataFrame()\n",
        "df_sets = pd.concat(all_sets, ignore_index=True) if all_sets else pd.DataFrame()\n",
        "df_impulse = pd.concat(all_impulse, ignore_index=True) if all_impulse else pd.DataFrame()\n",
        "df_ban_meta = pd.DataFrame(ban_meta_rows)\n",
        "\n",
        "df_workouts.head(), df_sets.head()\n"
      ],
      "metadata": {
        "id": "NFCZb7CrjLnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL 10 — (Python) Derive workoutlogs (exercise-level) + sessions (session-level)**"
      ],
      "metadata": {
        "id": "O6M2Sj8AjOcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# join plan -> per calcolare gapadherencescore a livello esercizio\n",
        "plan_key = [\"workoutid\",\"userid\",\"date\",\"sessiontag\",\"exerciseid\"]\n",
        "df_plan_keyed = df_plan[plan_key + [\"setsplanned\",\"repsmin\",\"repsmax\",\"rirtarget\"]].copy()\n",
        "\n",
        "# aggregate sets -> exercise\n",
        "gex = df_sets.groupby([\"workoutid\",\"userid\",\"date\",\"sessiontag\",\"exerciseid\"], as_index=False).agg(\n",
        "    setsdone=(\"set_index\",\"max\"),\n",
        "    repsdonetotal=(\"reps_done\",\"sum\"),\n",
        "    repsdoneavg=(\"reps_done\",\"mean\"),\n",
        "    loaddonekg=(\"load_done_kg\",\"median\"),\n",
        "    rpedone=(\"rpe_done\",\"mean\"),\n",
        "    loadintendedkg=(\"load_intended_kg\",\"median\"),\n",
        "    reps_target_avg=(\"reps_target\",\"mean\"),\n",
        ")\n",
        "\n",
        "df_logs = gex.merge(df_plan_keyed, on=plan_key, how=\"left\")\n",
        "\n",
        "# gapadherencescore (coerente con idea del tuo script: ratio carico/sets/reps) [file:1]\n",
        "carratio = (df_logs[\"loaddonekg\"] / df_logs[\"loadintendedkg\"]).replace([np.inf, -np.inf], np.nan).fillna(1.0)\n",
        "sratio = (df_logs[\"setsdone\"] / df_logs[\"setsplanned\"]).replace([np.inf, -np.inf], np.nan).fillna(1.0)\n",
        "# reps target “centrale” ~ media target\n",
        "repratio = (df_logs[\"repsdoneavg\"] / df_logs[\"reps_target_avg\"]).replace([np.inf, -np.inf], np.nan).fillna(1.0)\n",
        "\n",
        "gap = 0.45*carratio + 0.30*sratio + 0.25*repratio\n",
        "df_logs[\"gapadherencescore\"] = np.clip(gap, 0.3, 1.8).round(3)\n",
        "\n",
        "df_logs[\"repsdoneavg\"] = df_logs[\"repsdoneavg\"].round(2)\n",
        "df_logs[\"loaddonekg\"] = df_logs[\"loaddonekg\"].round(2)\n",
        "df_logs[\"rpedone\"] = df_logs[\"rpedone\"].round(2)\n",
        "\n",
        "# sessions (session-level)\n",
        "df_sessions = df_sets.groupby([\"workoutid\",\"userid\",\"date\",\"sessiontag\"], as_index=False).agg(\n",
        "    total_sets=(\"set_index\",\"count\"),\n",
        "    total_reps=(\"reps_done\",\"sum\"),\n",
        "    volume_kg=(\"load_done_kg\", lambda x: float(np.nansum(x.values))),  # solo somma load (non volume)\n",
        ")\n",
        "# volume vero = sum(load*reps)\n",
        "tmp = df_sets.copy()\n",
        "tmp[\"load_done_kg_0\"] = tmp[\"load_done_kg\"].fillna(0.0)\n",
        "tmp[\"volume_kg\"] = tmp[\"load_done_kg_0\"] * tmp[\"reps_done\"].astype(float)\n",
        "df_sessions = tmp.groupby([\"workoutid\",\"userid\",\"date\",\"sessiontag\"], as_index=False).agg(\n",
        "    total_sets=(\"set_index\",\"count\"),\n",
        "    total_reps=(\"reps_done\",\"sum\"),\n",
        "    volume_kg=(\"volume_kg\",\"sum\"),\n",
        "    sRPE=(\"rpe_done\",\"mean\")\n",
        ")\n",
        "df_sessions[\"sRPE\"] = df_sessions[\"sRPE\"].round(2)\n",
        "\n",
        "df_logs.head(), df_sessions.head()\n"
      ],
      "metadata": {
        "id": "bHKvVs7JjRHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL 11 — (Python) Compute Banister daily**"
      ],
      "metadata": {
        "id": "GJzKTmHhjR47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_banister_daily(cfg: CFG, df_users: pd.DataFrame, df_impulse: pd.DataFrame, df_ban_meta: pd.DataFrame):\n",
        "    # per ogni utente crea serie giornaliera sul range [start_u, end_u]\n",
        "    rows = []\n",
        "    for u in df_users.itertuples(index=False):\n",
        "        uid = int(u.userid)\n",
        "        start_u = date.fromisoformat(u.start_date)\n",
        "        end_u = date.fromisoformat(u.end_date)\n",
        "\n",
        "        meta = df_ban_meta[df_ban_meta[\"userid\"] == uid].iloc[0].to_dict()\n",
        "        tauF = float(meta[\"tauF\"]); tauD = float(meta[\"tauD\"])\n",
        "        betaF = float(meta[\"betaF\"]); betaD = float(meta[\"betaD\"])\n",
        "\n",
        "        days = [start_u + timedelta(days=i) for i in range((end_u - start_u).days + 1)]\n",
        "        days_iso = [d.isoformat() for d in days]\n",
        "\n",
        "        sub = df_impulse[df_impulse[\"userid\"] == uid].copy()\n",
        "        imp_map = dict(zip(sub[\"date\"].astype(str), sub[\"impulse\"].astype(float)))\n",
        "\n",
        "        uts = np.array([float(imp_map.get(d, 0.0)) for d in days_iso], dtype=float)\n",
        "        L = len(uts)\n",
        "        wF = exp_weights(L, tauF)\n",
        "        wD = exp_weights(L, tauD)\n",
        "\n",
        "        # calcolo cumulativo “naive” O(L^2) (ok per dataset medio); ottimizzabile se serve\n",
        "        F = np.array([float(np.sum(uts[:i+1][::-1] * wF[:i+1])) for i in range(L)], dtype=float)\n",
        "        D = np.array([float(np.sum(uts[:i+1][::-1] * wD[:i+1])) for i in range(L)], dtype=float)\n",
        "        P = betaF * F - betaD * D\n",
        "\n",
        "        for i, d in enumerate(days_iso):\n",
        "            rows.append({\n",
        "                \"userid\": uid,\n",
        "                \"date\": d,\n",
        "                \"impulse\": float(uts[i]),\n",
        "                \"F\": float(F[i]),\n",
        "                \"D\": float(D[i]),\n",
        "                \"P\": float(P[i]),\n",
        "                \"tauF\": tauF,\n",
        "                \"tauD\": tauD,\n",
        "                \"betaF\": betaF,\n",
        "                \"betaD\": betaD,\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "df_ban = compute_banister_daily(cfg, df_users, df_impulse, df_ban_meta)\n",
        "df_ban.head()\n"
      ],
      "metadata": {
        "id": "ouQvRYJEJF0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL 12 — (Python) Validations + Save CSV + Zip**"
      ],
      "metadata": {
        "id": "Pyg4jg9QjWsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_dataset(df_users, df_workouts, df_plan, df_sets, df_logs, df_sessions, df_ban):\n",
        "    checks = {}\n",
        "\n",
        "    # 1) date range per utente\n",
        "    u = df_users.copy()\n",
        "    u[\"start_date\"] = pd.to_datetime(u[\"start_date\"]).dt.date\n",
        "    u[\"end_date\"] = pd.to_datetime(u[\"end_date\"]).dt.date\n",
        "    s = df_sets.copy()\n",
        "    s[\"date\"] = pd.to_datetime(s[\"date\"]).dt.date\n",
        "\n",
        "    merged = s.merge(u[[\"userid\",\"start_date\",\"end_date\"]], on=\"userid\", how=\"left\")\n",
        "    checks[\"sets_in_range\"] = bool(((merged[\"date\"] >= merged[\"start_date\"]) & (merged[\"date\"] <= merged[\"end_date\"])).all())\n",
        "\n",
        "    # 2) set_id unico\n",
        "    checks[\"set_id_unique\"] = bool(df_sets[\"set_id\"].is_unique)\n",
        "\n",
        "    # 3) consistenza workoutstatus: se session skipped, non dovrebbero esserci set\n",
        "    w = df_workouts.copy()\n",
        "    sw = s.merge(w[[\"workoutid\",\"workoutstatus\"]], on=\"workoutid\", how=\"left\")\n",
        "    checks[\"no_sets_for_skipped\"] = bool((sw[sw[\"workoutstatus\"] == \"skipped\"].shape[0] == 0))\n",
        "\n",
        "    # 4) chiavi minime non nulle\n",
        "    required_cols = [\"userid\",\"date\",\"exerciseid\",\"set_index\"]\n",
        "    checks[\"sets_required_cols_nonnull\"] = bool(df_sets[required_cols].notnull().all().all())\n",
        "\n",
        "    return checks\n",
        "\n",
        "checks = validate_dataset(df_users, df_workouts, df_plan, df_sets, df_logs, df_sessions, df_ban)\n",
        "checks\n"
      ],
      "metadata": {
        "id": "lzvWdodjjZLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save\n",
        "df_users.to_csv(OUTDIR / \"users.csv\", index=False)\n",
        "df_ex.to_csv(OUTDIR / \"exercises.csv\", index=False)\n",
        "df_workouts.to_csv(OUTDIR / \"workouts.csv\", index=False)\n",
        "df_plan.to_csv(OUTDIR / \"workoutexercises.csv\", index=False)     # plan per esercizio (come prima) [file:1]\n",
        "df_sets.to_csv(OUTDIR / \"workout_sets.csv\", index=False)          # canonico set-level\n",
        "df_logs.to_csv(OUTDIR / \"workoutlogs.csv\", index=False)           # derivato exercise-level (compat) [file:1]\n",
        "df_sessions.to_csv(OUTDIR / \"sessions.csv\", index=False)          # derivato session-level\n",
        "df_ban.to_csv(OUTDIR / \"banisterdaily.csv\", index=False)          # compat concettuale [file:1]\n",
        "\n",
        "with open(OUTDIR / \"validation_checks.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(checks, f, indent=2)\n",
        "\n",
        "# Zip per download\n",
        "zip_name = f\"{cfg.outdir}.zip\"\n",
        "!zip -r {zip_name} {cfg.outdir}\n",
        "print(\"DONE:\", zip_name)\n"
      ],
      "metadata": {
        "id": "KN2DE0sJjfFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EDA 1 utente a caso**"
      ],
      "metadata": {
        "id": "rpf90ctHZBLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DATA_DIR = OUTDIR  # se stai nello stesso notebook; altrimenti Path(\"data_synth_setlevel\")\n"
      ],
      "metadata": {
        "id": "TPrmetFiYI3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ban = pd.read_csv(DATA_DIR / \"banisterdaily.csv\")\n",
        "sets = pd.read_csv(DATA_DIR / \"workout_sets.csv\")\n",
        "workouts = pd.read_csv(DATA_DIR / \"workouts.csv\")\n",
        "\n",
        "# parsing date\n",
        "ban[\"date\"] = pd.to_datetime(ban[\"date\"])\n",
        "sets[\"date\"] = pd.to_datetime(sets[\"date\"])\n",
        "workouts[\"date\"] = pd.to_datetime(workouts[\"date\"])\n",
        "\n",
        "# scegli utente a caso tra quelli presenti nei set (così siamo sicuri che abbia attività)\n",
        "uid = int(np.random.choice(sets[\"userid\"].unique(), 1)[0])\n",
        "uid\n"
      ],
      "metadata": {
        "id": "1XJ8rCfuZPes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_ban = ban[ban[\"userid\"] == uid].sort_values(\"date\").copy()\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
        "\n",
        "ax[0].plot(sub_ban[\"date\"], sub_ban[\"P\"], linewidth=1.6)\n",
        "ax[0].set_title(f\"User {uid} — Banister P over time\")\n",
        "ax[0].set_ylabel(\"P\")\n",
        "\n",
        "ax[1].bar(sub_ban[\"date\"], sub_ban[\"impulse\"], width=1.0)\n",
        "ax[1].set_title(\"Daily impulse\")\n",
        "ax[1].set_ylabel(\"Impulse\")\n",
        "ax[1].set_xlabel(\"Date\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oTE6fnrKZROC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_sets = sets[sets[\"userid\"] == uid].copy()\n",
        "\n",
        "# volume set-level: load*reps (gestione NaN)\n",
        "sub_sets[\"load0\"] = sub_sets[\"load_done_kg\"].fillna(0.0)\n",
        "sub_sets[\"volume_kg\"] = sub_sets[\"load0\"] * sub_sets[\"reps_done\"].astype(float)\n",
        "\n",
        "# volume per sessione (workoutid)\n",
        "vol_session = (sub_sets.groupby([\"workoutid\",\"date\"], as_index=False)[\"volume_kg\"]\n",
        "               .sum()\n",
        "               .sort_values(\"date\"))\n",
        "\n",
        "# volume per settimana (weekindex_user se presente, altrimenti calcolo ISO week)\n",
        "if \"weekindex_user\" in sub_sets.columns:\n",
        "    vol_week = sub_sets.groupby(\"weekindex_user\", as_index=False)[\"volume_kg\"].sum()\n",
        "    x_week = vol_week[\"weekindex_user\"]\n",
        "else:\n",
        "    vol_session[\"iso_week\"] = vol_session[\"date\"].dt.isocalendar().week.astype(int)\n",
        "    vol_week = vol_session.groupby(\"iso_week\", as_index=False)[\"volume_kg\"].sum()\n",
        "    x_week = vol_week[\"iso_week\"]\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, figsize=(12, 6), sharex=False)\n",
        "\n",
        "ax[0].plot(vol_session[\"date\"], vol_session[\"volume_kg\"], marker=\"o\", linewidth=1.2)\n",
        "ax[0].set_title(f\"User {uid} — Session volume (kg)\")\n",
        "ax[0].set_ylabel(\"Volume (kg)\")\n",
        "ax[0].grid(True, alpha=0.25)\n",
        "\n",
        "ax[1].bar(x_week, vol_week[\"volume_kg\"])\n",
        "ax[1].set_title(\"Weekly volume (sum)\")\n",
        "ax[1].set_ylabel(\"Volume (kg)\")\n",
        "ax[1].set_xlabel(\"Week index\")\n",
        "ax[1].grid(True, alpha=0.25)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3xIp42hsZS5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_w = workouts[workouts[\"userid\"] == uid].copy()\n",
        "\n",
        "print(\"Sessions:\", len(sub_w))\n",
        "print(\"Done:\", (sub_w[\"workoutstatus\"] == \"done\").mean())\n",
        "print(\"\\nSession tags counts:\")\n",
        "print(sub_w[\"sessiontag\"].value_counts(dropna=False).head(10))\n"
      ],
      "metadata": {
        "id": "S5mxbX-wZVIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path(\"data_synth_setlevel\")  # cambia se serve\n",
        "\n",
        "workouts = pd.read_csv(DATA_DIR / \"workouts.csv\")\n",
        "\n",
        "# conteggi e percentuali\n",
        "counts = workouts[\"workoutstatus\"].value_counts(dropna=False)\n",
        "pct = workouts[\"workoutstatus\"].value_counts(dropna=False, normalize=True)\n",
        "\n",
        "print(pd.concat([counts, pct], axis=1, keys=[\"count\", \"pct\"]).round(4))\n",
        "\n",
        "sessions = pd.read_csv(DATA_DIR / \"sessions.csv\")\n",
        "\n",
        "# workoutid presenti in sessions ma non in workouts\n",
        "missing_wid = set(sessions[\"workoutid\"]) - set(workouts[\"workoutid\"])\n",
        "print(\"workoutid in sessions but not in workouts:\", len(missing_wid))\n",
        "\n",
        "# join per vedere lo status associato alle session\n",
        "s_join = sessions.merge(workouts[[\"workoutid\",\"workoutstatus\",\"userid\",\"date\"]],\n",
        "                        on=\"workoutid\", how=\"left\", validate=\"m:1\")\n",
        "\n",
        "print(s_join[\"workoutstatus\"].value_counts(dropna=False, normalize=True).round(4))\n",
        "\n",
        "n_workouts = len(workouts)\n",
        "n_sessions = len(sessions)\n",
        "\n",
        "print(\"workouts:\", n_workouts)\n",
        "print(\"sessions:\", n_sessions)\n",
        "print(\"sessions/workouts:\", n_sessions / n_workouts)\n"
      ],
      "metadata": {
        "id": "bS4q5LVve4rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Serie di celle pronte (Pandas + Seaborn/Matplotlib) per avere un “cruscotto” rapido della distribuzione dei parametri e di alcune metriche derivate (skip-rate, volume, RPE, ecc.)."
      ],
      "metadata": {
        "id": "FdEbFWgQ_sRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL A — Import + load CSV**"
      ],
      "metadata": {
        "id": "taOvxRfZ_052"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "DATA_DIR = OUTDIR if \"OUTDIR\" in globals() else Path(\"data_synth_setlevel\")  # cambia se serve\n",
        "\n",
        "df_users   = pd.read_csv(DATA_DIR / \"users.csv\")\n",
        "df_workouts= pd.read_csv(DATA_DIR / \"workouts.csv\")\n",
        "df_sets    = pd.read_csv(DATA_DIR / \"workout_sets.csv\")\n",
        "\n",
        "# opzionali (se li hai)\n",
        "ban_path = DATA_DIR / \"banisterdaily.csv\"\n",
        "df_ban = pd.read_csv(ban_path) if ban_path.exists() else None\n",
        "\n",
        "for df in [df_users, df_workouts, df_sets]:\n",
        "    if \"date\" in df.columns:\n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n"
      ],
      "metadata": {
        "id": "w1RB9lmv_zpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL B — Metriche per utente (sunto “alto livello”)**"
      ],
      "metadata": {
        "id": "WGRyXb0Z_8R8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# skip-rate per utente\n",
        "w = df_workouts.copy()\n",
        "w[\"is_skipped\"] = (w[\"workoutstatus\"] == \"skipped\").astype(int)\n",
        "w[\"is_done\"] = (w[\"workoutstatus\"] == \"done\").astype(int)\n",
        "\n",
        "per_user_workouts = (w.groupby(\"userid\", as_index=False)\n",
        "                     .agg(workouts_total=(\"workoutstatus\",\"size\"),\n",
        "                          workouts_done=(\"is_done\",\"sum\"),\n",
        "                          workouts_skipped=(\"is_skipped\",\"sum\"),\n",
        "                          skip_rate=(\"is_skipped\",\"mean\")))\n",
        "\n",
        "# volume per utente (da set-level): volume = load_done_kg * reps_done (NaN load -> 0)\n",
        "s = df_sets.copy()\n",
        "s[\"load0\"] = s[\"load_done_kg\"].fillna(0.0)\n",
        "s[\"volume_kg\"] = s[\"load0\"] * s[\"reps_done\"].astype(float)\n",
        "\n",
        "per_user_volume = (s.groupby(\"userid\", as_index=False)\n",
        "                   .agg(sets_total=(\"set_id\",\"count\"),\n",
        "                        volume_total_kg=(\"volume_kg\",\"sum\"),\n",
        "                        volume_mean_set_kg=(\"volume_kg\",\"mean\"),\n",
        "                        rpe_mean=(\"rpe_done\",\"mean\"),\n",
        "                        rpe_std=(\"rpe_done\",\"std\")))\n",
        "\n",
        "# merge con user params (usa il nome colonna giusto della tua label)\n",
        "label_col = \"experience_label\" if \"experience_label\" in df_users.columns else (\n",
        "            \"experiencedeclared\" if \"experiencedeclared\" in df_users.columns else None)\n",
        "\n",
        "keep_user_cols = [\"userid\"] + ([label_col] if label_col else [])\n",
        "df_summary = (df_users[keep_user_cols]\n",
        "              .merge(per_user_workouts, on=\"userid\", how=\"left\")\n",
        "              .merge(per_user_volume, on=\"userid\", how=\"left\"))\n",
        "\n",
        "df_summary.head()\n"
      ],
      "metadata": {
        "id": "HfE8mGjE__Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL C — Distribuzioni parametri “puri” (users.csv)**"
      ],
      "metadata": {
        "id": "fmXl7u-SAAgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scegli parametri numerici da plottare (metti quelli che hai nel tuo users.csv)\n",
        "candidate = [\n",
        "    \"weeklyfreqdeclared\", \"Aadherence\", \"fatiguesensitivity\",\n",
        "    \"alphaprogression\", \"rpenoisesd\", \"rpereportbias\"\n",
        "]\n",
        "param_cols = [c for c in candidate if c in df_users.columns]\n",
        "\n",
        "n = len(param_cols)\n",
        "fig, axes = plt.subplots(n, 1, figsize=(12, 3*n))\n",
        "if n == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for ax, col in zip(axes, param_cols):\n",
        "    sns.histplot(data=df_users, x=col, kde=True, ax=ax)\n",
        "    ax.set_title(f\"Distribuzione: {col}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XejJWt1FAD6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL D — Confronto per livello (violin/box)**"
      ],
      "metadata": {
        "id": "D1v6NLsVAFRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if label_col:\n",
        "    # metriche “sintetiche” per confrontare i gruppi\n",
        "    cols_to_compare = [\"skip_rate\", \"workouts_total\", \"volume_total_kg\", \"rpe_mean\"]\n",
        "    cols_to_compare = [c for c in cols_to_compare if c in df_summary.columns]\n",
        "\n",
        "    for col in cols_to_compare:\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        sns.violinplot(data=df_summary, x=label_col, y=col, inner=\"quartile\")\n",
        "        plt.title(f\"{col} per {label_col}\")\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "i8vzDbn8AHOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL E — Correlazioni (heatmap)**"
      ],
      "metadata": {
        "id": "0MWhqvotAIpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num = df_summary.select_dtypes(include=[np.number]).copy()\n",
        "corr = num.corr()\n",
        "\n",
        "plt.figure(figsize=(12, 9))\n",
        "sns.heatmap(corr, cmap=\"coolwarm\", center=0, annot=False)\n",
        "plt.title(\"Correlation heatmap (user-level metrics)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5FAVH4ehAPXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL F — Banister: distribuzione di tau e P**"
      ],
      "metadata": {
        "id": "53azEsqJAQ5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if df_ban is not None:\n",
        "    # 1) distribuzione tau (un valore per utente: prendiamo la prima riga per uid)\n",
        "    ban_meta = (df_ban.sort_values([\"userid\",\"date\"])\n",
        "                     .groupby(\"userid\", as_index=False)\n",
        "                     .first()[[\"userid\",\"tauF\",\"tauD\",\"betaF\",\"betaD\"]])\n",
        "\n",
        "    fig, ax = plt.subplots(2, 1, figsize=(12, 6))\n",
        "    sns.histplot(ban_meta[\"tauF\"], kde=True, ax=ax[0]); ax[0].set_title(\"tauF distribution\")\n",
        "    sns.histplot(ban_meta[\"tauD\"], kde=True, ax=ax[1]); ax[1].set_title(\"tauD distribution\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "    # 2) P: distribuzione globale e per livello (se hai label)\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    sns.histplot(data=df_ban, x=\"P\", bins=60)\n",
        "    plt.title(\"Distribuzione globale P (tutte le righe banisterdaily)\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "K5jHyoYpATF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL G - Load + pick random user (COMPLETA)**"
      ],
      "metadata": {
        "id": "WMIzWCpQTlP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = OUTDIR if \"OUTDIR\" in globals() else Path(\"data_synth_setlevel\")  # cambia se serve\n",
        "\n",
        "ban = pd.read_csv(DATA_DIR / \"banisterdaily.csv\")\n",
        "workouts = pd.read_csv(DATA_DIR / \"workouts.csv\")\n",
        "sets = pd.read_csv(DATA_DIR / \"workout_sets.csv\")\n",
        "\n",
        "ban[\"date\"] = pd.to_datetime(ban[\"date\"])\n",
        "workouts[\"date\"] = pd.to_datetime(workouts[\"date\"])\n",
        "sets[\"date\"] = pd.to_datetime(sets[\"date\"])\n",
        "\n",
        "# Prendo un utente che abbia almeno N sessioni done (così il grafico è interessante)\n",
        "N_MIN_DONE = 15\n",
        "done_counts = (workouts[workouts[\"workoutstatus\"] == \"done\"]\n",
        "               .groupby(\"userid\").size())\n",
        "\n",
        "eligible = done_counts[done_counts >= N_MIN_DONE].index.values\n",
        "if len(eligible) == 0:\n",
        "    eligible = workouts[\"userid\"].unique()\n",
        "\n",
        "uid = int(np.random.choice(eligible, 1)[0])\n",
        "uid\n"
      ],
      "metadata": {
        "id": "cQTarXASTk17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL G.1 — EDA + Grafici utente (COMPLETA)**"
      ],
      "metadata": {
        "id": "lV2TLhxlTz-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- subset dati utente ---\n",
        "ban_u = ban[ban[\"userid\"] == uid].sort_values(\"date\").copy()\n",
        "work_u = workouts[(workouts[\"userid\"] == uid) & (workouts[\"workoutstatus\"] == \"done\")].sort_values(\"date\").copy()\n",
        "sets_u = sets[sets[\"userid\"] == uid].sort_values(\"date\").copy()\n",
        "\n",
        "# --- feature set-level derivate ---\n",
        "sets_u[\"load0\"] = sets_u[\"load_done_kg\"].fillna(0.0)\n",
        "sets_u[\"volume_kg\"] = sets_u[\"load0\"] * sets_u[\"reps_done\"].astype(float)\n",
        "\n",
        "# aggregazioni per giorno\n",
        "daily_sets = (sets_u.groupby(\"date\", as_index=False)\n",
        "              .agg(sets=(\"set_id\", \"count\") if \"set_id\" in sets_u.columns else (\"reps_done\",\"size\"),\n",
        "                   reps_total=(\"reps_done\", \"sum\"),\n",
        "                   load_mean=(\"load_done_kg\", \"mean\"),\n",
        "                   load_max=(\"load_done_kg\", \"max\"),\n",
        "                   volume_total=(\"volume_kg\", \"sum\"),\n",
        "                   rpe_mean=(\"rpe_done\", \"mean\")))\n",
        "\n",
        "# restringo il banister al periodo effettivo dove c'è attività (se preferisci)\n",
        "if len(daily_sets) > 0:\n",
        "    d0, d1 = daily_sets[\"date\"].min(), daily_sets[\"date\"].max()\n",
        "    ban_u_plot = ban_u[(ban_u[\"date\"] >= d0) & (ban_u[\"date\"] <= d1)].copy()\n",
        "else:\n",
        "    ban_u_plot = ban_u.copy()\n",
        "\n",
        "# --- EDA stampata (rapida) ---\n",
        "print(f\"USERID: {uid}\")\n",
        "print(f\"Periodo banister: {ban_u['date'].min().date()} -> {ban_u['date'].max().date()}  (righe={len(ban_u)})\")\n",
        "print(f\"Workout DONE: {len(work_u)}\")\n",
        "print(f\"Set rows: {len(sets_u)}\")\n",
        "print(\"\\nBanister summary (P, F, D, impulse):\")\n",
        "print(ban_u_plot[[\"P\",\"F\",\"D\",\"impulse\"]].describe().round(3))\n",
        "\n",
        "print(\"\\nDaily training summary:\")\n",
        "if len(daily_sets) > 0:\n",
        "    print(daily_sets[[\"sets\",\"reps_total\",\"load_mean\",\"load_max\",\"volume_total\",\"rpe_mean\"]].describe().round(3))\n",
        "else:\n",
        "    print(\"Nessun set trovato per questo utente.\")\n",
        "\n",
        "# --- GRAFICO 1: Banister (P, F, D) + impulse ---\n",
        "fig, ax = plt.subplots(4, 1, figsize=(14, 10), sharex=True)\n",
        "\n",
        "ax[0].plot(ban_u_plot[\"date\"], ban_u_plot[\"P\"], linewidth=1.6)\n",
        "ax[0].set_title(f\"User {uid} — Banister P\")\n",
        "ax[0].set_ylabel(\"P\")\n",
        "\n",
        "ax[1].plot(ban_u_plot[\"date\"], ban_u_plot[\"F\"], linewidth=1.2)\n",
        "ax[1].set_title(\"Fitness (F)\")\n",
        "ax[1].set_ylabel(\"F\")\n",
        "\n",
        "ax[2].plot(ban_u_plot[\"date\"], ban_u_plot[\"D\"], linewidth=1.2)\n",
        "ax[2].set_title(\"Fatigue (D)\")\n",
        "ax[2].set_ylabel(\"D\")\n",
        "\n",
        "ax[3].bar(ban_u_plot[\"date\"], ban_u_plot[\"impulse\"], width=1.0)\n",
        "ax[3].set_title(\"Daily impulse\")\n",
        "ax[3].set_ylabel(\"Impulse\")\n",
        "ax[3].set_xlabel(\"Date\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- GRAFICO 2: Allenamenti nel tempo (carico, reps, volume) ---\n",
        "# Nota: \"carico\" qui è load_mean e load_max per giorno (dai set loggati)\n",
        "fig, ax = plt.subplots(3, 1, figsize=(14, 9), sharex=True)\n",
        "\n",
        "if len(daily_sets) > 0:\n",
        "    ax[0].plot(daily_sets[\"date\"], daily_sets[\"load_mean\"], marker=\"o\", linewidth=1.2, label=\"Mean load (kg)\")\n",
        "    ax[0].plot(daily_sets[\"date\"], daily_sets[\"load_max\"], marker=\".\", linewidth=1.0, label=\"Max load (kg)\")\n",
        "    ax[0].set_title(f\"User {uid} — Carichi (da set loggati)\")\n",
        "    ax[0].set_ylabel(\"kg\")\n",
        "    ax[0].legend()\n",
        "\n",
        "    ax[1].bar(daily_sets[\"date\"], daily_sets[\"reps_total\"], width=1.0)\n",
        "    ax[1].set_title(\"Reps totali per giorno\")\n",
        "    ax[1].set_ylabel(\"reps\")\n",
        "\n",
        "    ax[2].bar(daily_sets[\"date\"], daily_sets[\"volume_total\"], width=1.0)\n",
        "    ax[2].set_title(\"Volume totale per giorno (kg)\")\n",
        "    ax[2].set_ylabel(\"volume (kg)\")\n",
        "    ax[2].set_xlabel(\"Date\")\n",
        "else:\n",
        "    ax[0].text(0.5, 0.5, \"No daily sets to plot\", ha=\"center\", va=\"center\")\n",
        "    ax[1].axis(\"off\")\n",
        "    ax[2].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- (opzionale) overlay: volume vs P (stesso grafico, due assi) ---\n",
        "if len(daily_sets) > 0 and len(ban_u_plot) > 0:\n",
        "    merged = pd.merge(\n",
        "        ban_u_plot[[\"date\",\"P\"]],\n",
        "        daily_sets[[\"date\",\"volume_total\"]],\n",
        "        on=\"date\", how=\"left\"\n",
        "    ).fillna({\"volume_total\": 0.0})\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(14, 4))\n",
        "    ax1.plot(merged[\"date\"], merged[\"P\"], color=\"tab:blue\", linewidth=1.6)\n",
        "    ax1.set_ylabel(\"P\", color=\"tab:blue\")\n",
        "    ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.bar(merged[\"date\"], merged[\"volume_total\"], alpha=0.25, color=\"tab:orange\", width=1.0)\n",
        "    ax2.set_ylabel(\"Daily volume (kg)\", color=\"tab:orange\")\n",
        "    ax2.tick_params(axis=\"y\", labelcolor=\"tab:orange\")\n",
        "\n",
        "    ax1.set_title(f\"User {uid} — Overlay: P vs Volume\")\n",
        "    ax1.set_xlabel(\"Date\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "WZkVwPs1T4YW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}