{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 1 - CONFIGURATION**"
      ],
      "metadata": {
        "id": "_inbZcQPXW8h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGDMPit6XN76",
        "outputId": "9ac3f4b3-54ec-4bc9-8ad4-2f9a6b678eb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STATUS MODULE - FEATURE ENGINEERING v2.2\n",
            "================================================================================\n",
            "Timestamp: 2026-02-16 00:44:01\n",
            "\n",
            "================================================================================\n",
            "SECTION 1: CONFIGURATION\n",
            "================================================================================\n",
            "\n",
            "[OK] Data directory: data/synth_set_level_v2\n",
            "[OK] Model output directory: models\n",
            "[OK] Selected features: 7\n",
            "[OK] Random seed: 42\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# STATUS MODULE - FEATURE ENGINEERING v2.2\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "#\n",
        "# OBIETTIVO:\n",
        "# Preparare dataset user-level per classificazione experience (Beginner/Intermediate/Advanced)\n",
        "#\n",
        "# INPUT:\n",
        "#   - data/synth_set_level_v2/users_sampled.csv (510 users)\n",
        "#   - data/synth_set_level_v2/workout_sets_sampled.csv (1.56M sets)\n",
        "#   - data/synth_set_level_v2/workouts_sampled.csv (106k workouts)\n",
        "#\n",
        "# OUTPUT:\n",
        "#   - models/status_preprocessed_v2.2.pkl (X_train, X_test, y_train, y_test, scaler)\n",
        "#   - models/status_feature_metadata_v2.2.json (feature info per interpretability)\n",
        "#\n",
        "# FEATURE SELECTION v2.2 (7 features, leakage-free):\n",
        "#   - reps_mean, rpe_mean, total_sets, acwr_mean, spike_weeks_count,\n",
        "#     load_progression, skip_rate\n",
        "#\n",
        "# RIMOSSI per leakage/low discriminative power:\n",
        "#   - consistency_score (correlation 1.000, deterministic)\n",
        "#   - load_mean (correlation 0.803, borderline)\n",
        "#   - observed_freq_weekly (Cohen's d 0.04, non discrimina)\n",
        "#\n",
        "# VERSIONE: 2.2\n",
        "# AUTORE: Alessandro Ambrosio\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from scipy import stats\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STATUS MODULE - FEATURE ENGINEERING v2.2\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print()\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 1: CONFIGURATION & PATHS\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 1: CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Paths\n",
        "DATADIR = Path('data/synth_set_level_v2')\n",
        "MODELDIR = Path('models')\n",
        "MODELDIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Feature selection v2.2 (leakage-free)\n",
        "SELECTED_FEATURES = [\n",
        "    'reps_mean',\n",
        "    'rpe_mean',\n",
        "    'total_sets',\n",
        "    'acwr_mean',\n",
        "    'spike_weeks_count',\n",
        "    'load_progression',\n",
        "    'skip_rate'\n",
        "]\n",
        "\n",
        "# Seeds (reproducibility)\n",
        "SEED_SPLIT = 42\n",
        "SEED_CV = 42\n",
        "\n",
        "# Target encoding\n",
        "TARGET_ENCODING = {'Beginner': 0, 'Intermediate': 1, 'Advanced': 2}\n",
        "\n",
        "print(f\"\\n[OK] Data directory: {DATADIR}\")\n",
        "print(f\"[OK] Model output directory: {MODELDIR}\")\n",
        "print(f\"[OK] Selected features: {len(SELECTED_FEATURES)}\")\n",
        "print(f\"[OK] Random seed: {SEED_SPLIT}\")\n",
        "print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 2 - LOAD RAW DATA**"
      ],
      "metadata": {
        "id": "Pe7iKdAFXk3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 2: LOAD RAW DATA\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 2: LOAD RAW DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load datasets\n",
        "df_users = pd.read_csv(DATADIR / 'users_sampled.csv')\n",
        "df_sets = pd.read_csv(DATADIR / 'workout_sets_sampled.csv')\n",
        "df_workouts = pd.read_csv(DATADIR / 'workouts_sampled.csv')\n",
        "\n",
        "# Parse dates\n",
        "df_sets['date'] = pd.to_datetime(df_sets['date'])\n",
        "df_workouts['date'] = pd.to_datetime(df_workouts['date'])\n",
        "\n",
        "print(f\"\\n[OK] Users loaded: {len(df_users):,}\")\n",
        "print(f\"[OK] Sets loaded: {len(df_sets):,}\")\n",
        "print(f\"[OK] Workouts loaded: {len(df_workouts):,}\")\n",
        "\n",
        "# Target distribution\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"TARGET DISTRIBUTION\")\n",
        "print(\"-\"*80)\n",
        "target_dist = df_users['experience_label'].value_counts().sort_index()\n",
        "for label, count in target_dist.items():\n",
        "    print(f\"{label:12s}: {count:3d} ({count/len(df_users)*100:5.1f}%)\")\n",
        "\n",
        "print()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uilsO48XrUc",
        "outputId": "3070c6f9-ac1f-4a45-8abd-4b213902dfb2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SECTION 2: LOAD RAW DATA\n",
            "================================================================================\n",
            "\n",
            "[OK] Users loaded: 510\n",
            "[OK] Sets loaded: 1,566,944\n",
            "[OK] Workouts loaded: 106,571\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TARGET DISTRIBUTION\n",
            "--------------------------------------------------------------------------------\n",
            "Advanced    : 170 ( 33.3%)\n",
            "Beginner    : 170 ( 33.3%)\n",
            "Intermediate: 170 ( 33.3%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CELLA 3 - USER-LEVEL AGGREGATIONS**"
      ],
      "metadata": {
        "id": "J6x8tVk4XlBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 3: USER-LEVEL AGGREGATIONS\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 3: USER-LEVEL AGGREGATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n[1/7] Basic workout statistics...\")\n",
        "\n",
        "# Basic stats\n",
        "user_agg_basic = df_sets.groupby('user_id').agg({\n",
        "    'load_done_kg': ['mean', 'std', 'min', 'max'],\n",
        "    'reps_done': ['mean', 'std'],\n",
        "    'rpe_done': ['mean', 'std'],\n",
        "    'set_id': 'count'  # Total sets\n",
        "}).reset_index()\n",
        "\n",
        "# Flatten columns\n",
        "user_agg_basic.columns = [\n",
        "    'user_id', 'load_mean', 'load_std', 'load_min', 'load_max',\n",
        "    'reps_mean', 'reps_std', 'rpe_mean', 'rpe_std', 'total_sets'\n",
        "]\n",
        "\n",
        "print(f\"  [OK] Aggregated {len(user_agg_basic)} users\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"[2/7] ACWR statistics...\")\n",
        "\n",
        "user_acwr = df_sets.groupby('user_id')['acwr'].mean().reset_index()\n",
        "user_acwr.columns = ['user_id', 'acwr_mean']\n",
        "\n",
        "print(f\"  [OK] ACWR computed for {len(user_acwr)} users\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# [3/7] Spike weeks count\n",
        "\n",
        "# Normalize week_type (avoid malformed values)\n",
        "df_sets['week_type'] = (\n",
        "    df_sets['week_type']\n",
        "    .astype(str).str.strip().str.lower()\n",
        "    .replace({'sp': 'spike'})\n",
        ")\n",
        "\n",
        "# Build a week identifier (choose a convention and keep it consistent with EDA)\n",
        "df_sets['week_id'] = df_sets['date'].dt.to_period('W-SUN')\n",
        "\n",
        "spike_counts = (\n",
        "    df_sets[df_sets['week_type'].eq('spike')]\n",
        "    .drop_duplicates(['user_id', 'week_id'])\n",
        "    .groupby('user_id')\n",
        "    .size()\n",
        "    .reset_index(name='spike_weeks_count')\n",
        ")\n",
        "\n",
        "print(f\"  [OK] Spike weeks counted for {len(spike_counts)} users\")\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"[4/7] Workout frequency...\")\n",
        "\n",
        "# Count workouts per user\n",
        "workout_counts = df_workouts.groupby('user_id').size().reset_index(name='total_workouts')\n",
        "\n",
        "# Calculate duration\n",
        "user_duration = df_workouts.groupby('user_id')['date'].agg(['min', 'max']).reset_index()\n",
        "user_duration['duration_days'] = (user_duration['max'] - user_duration['min']).dt.days + 1\n",
        "user_duration['duration_weeks'] = user_duration['duration_days'] / 7.0\n",
        "\n",
        "# Observed frequency (workouts/week)\n",
        "workout_counts = workout_counts.merge(user_duration[['user_id', 'duration_weeks']], on='user_id')\n",
        "workout_counts['observed_freq_weekly'] = workout_counts['total_workouts'] / workout_counts['duration_weeks']\n",
        "\n",
        "print(f\"  [OK] Frequency computed for {len(workout_counts)} users\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"[5/7] Skip rate...\")\n",
        "\n",
        "skip_stats = df_workouts.groupby('user_id')['workout_status'].apply(\n",
        "    lambda x: (x == 'skipped').sum() / len(x) if len(x) > 0 else 0.0\n",
        ").reset_index(name='skip_rate')\n",
        "\n",
        "print(f\"  [OK] Skip rate computed for {len(skip_stats)} users\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"[6/7] Load progression (slope regression)...\")\n",
        "\n",
        "def calculate_load_progression(user_df):\n",
        "    \"\"\"\n",
        "    Calculate slope of load over time using linear regression.\n",
        "    Returns monthly progression rate (1.0 = no change, 1.1 = +10%/month).\n",
        "    \"\"\"\n",
        "    if len(user_df) < 10:\n",
        "        return 1.0\n",
        "\n",
        "    user_df = user_df.sort_values('date').reset_index(drop=True)\n",
        "    user_df = user_df.dropna(subset=['load_done_kg'])\n",
        "\n",
        "    if len(user_df) < 10:\n",
        "        return 1.0\n",
        "\n",
        "    # Time index (days from start)\n",
        "    user_df['days_from_start'] = (user_df['date'] - user_df['date'].min()).dt.days\n",
        "\n",
        "    # Linear regression\n",
        "    slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
        "        user_df['days_from_start'],\n",
        "        user_df['load_done_kg']\n",
        "    )\n",
        "\n",
        "    # Normalize: slope per 30 days (monthly rate)\n",
        "    slope_monthly = slope * 30\n",
        "\n",
        "    # Convert to multiplier (1.0 = no change, 1.1 = +10% per month)\n",
        "    if intercept > 0:\n",
        "        progression = 1.0 + (slope_monthly / intercept)\n",
        "    else:\n",
        "        progression = 1.0\n",
        "\n",
        "    # Clip to realistic range [0.5, 2.0]\n",
        "    return float(np.clip(progression, 0.5, 2.0))\n",
        "\n",
        "load_progressions = []\n",
        "for uid in user_agg_basic['user_id']:\n",
        "    user_sets = df_sets[df_sets['user_id'] == uid]\n",
        "    prog = calculate_load_progression(user_sets)\n",
        "    load_progressions.append({'user_id': uid, 'load_progression': prog})\n",
        "\n",
        "load_prog_df = pd.DataFrame(load_progressions)\n",
        "\n",
        "print(f\"  [OK] Load progression computed for {len(load_prog_df)} users\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"[7/7] Merging all aggregations...\")\n",
        "\n",
        "# Merge all\n",
        "user_agg = user_agg_basic.copy()\n",
        "user_agg = user_agg.merge(user_acwr, on='user_id', how='left')\n",
        "user_agg = user_agg.merge(spike_counts, on='user_id', how='left')\n",
        "user_agg = user_agg.merge(workout_counts[['user_id', 'observed_freq_weekly']], on='user_id', how='left')\n",
        "user_agg = user_agg.merge(skip_stats, on='user_id', how='left')\n",
        "user_agg = user_agg.merge(load_prog_df, on='user_id', how='left')\n",
        "\n",
        "# Fill missing spike_weeks_count with 0\n",
        "user_agg['spike_weeks_count'] = user_agg['spike_weeks_count'].fillna(0).astype(int)\n",
        "\n",
        "# Merge target\n",
        "user_agg = user_agg.merge(\n",
        "    df_users[['user_id', 'experience_label']],\n",
        "    on='user_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "print(f\"  [OK] Final dataset: {user_agg.shape}\")\n",
        "print()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSpaBR7JXuKk",
        "outputId": "ef573f80-b072-47c2-e730-84ca1df42b0b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SECTION 3: USER-LEVEL AGGREGATIONS\n",
            "================================================================================\n",
            "\n",
            "[1/7] Basic workout statistics...\n",
            "  [OK] Aggregated 510 users\n",
            "[2/7] ACWR statistics...\n",
            "  [OK] ACWR computed for 510 users\n",
            "  [OK] Spike weeks counted for 508 users\n",
            "[4/7] Workout frequency...\n",
            "  [OK] Frequency computed for 510 users\n",
            "[5/7] Skip rate...\n",
            "  [OK] Skip rate computed for 510 users\n",
            "[6/7] Load progression (slope regression)...\n",
            "  [OK] Load progression computed for 510 users\n",
            "[7/7] Merging all aggregations...\n",
            "  [OK] Final dataset: (510, 16)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 4 - DERIVED FEATURES**"
      ],
      "metadata": {
        "id": "JJMjhK_7XlFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 4: FEATURE ENGINEERING - DERIVED FEATURES (Optional)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 4: DERIVED FEATURES (Optional, for exploration)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Composite features (NOT used in final model, for reference only)\n",
        "user_agg['volume_score'] = user_agg['total_sets'] * user_agg['observed_freq_weekly']\n",
        "user_agg['intensity_score'] = user_agg['load_mean'] * user_agg['reps_mean']\n",
        "user_agg['rpe_volume_interaction'] = user_agg['rpe_mean'] * user_agg['reps_mean']\n",
        "\n",
        "# Ratio features\n",
        "user_agg['load_variability'] = user_agg['load_std'] / user_agg['load_mean']\n",
        "user_agg['rpe_variability'] = user_agg['rpe_std'] / user_agg['rpe_mean']\n",
        "\n",
        "# Spike rate (normalized)\n",
        "user_agg['spike_rate'] = user_agg['spike_weeks_count'] / (user_agg['total_sets'] / 10)  # per 10 sets\n",
        "\n",
        "print(f\"\\n[OK] Derived features created: 6\")\n",
        "print(\"  Note: These are NOT used in final model v2.2 (for reference/exploration only)\")\n",
        "print()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx0GTQF7Xy1F",
        "outputId": "1f12faa6-1e81-4357-fea1-f3baf71213ac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SECTION 4: DERIVED FEATURES (Optional, for exploration)\n",
            "================================================================================\n",
            "\n",
            "[OK] Derived features created: 6\n",
            "  Note: These are NOT used in final model v2.2 (for reference/exploration only)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 5 - FEATURE SELECTION & VALIDATION**"
      ],
      "metadata": {
        "id": "79EqlgP1XlJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 5: FEATURE SELECTION & VALIDATION\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 5: FEATURE SELECTION & VALIDATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Check all selected features exist\n",
        "print(\"\\n[1/3] Verifying selected features exist...\")\n",
        "missing_features = [f for f in SELECTED_FEATURES if f not in user_agg.columns]\n",
        "\n",
        "if missing_features:\n",
        "    print(f\"  [!]  Missing features: {missing_features}\")\n",
        "    raise ValueError(\"Some selected features not found in aggregated dataset\")\n",
        "else:\n",
        "    print(f\"  [OK] All {len(SELECTED_FEATURES)} features present\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n[2/3] Checking for missing values...\")\n",
        "\n",
        "missing_counts = user_agg[SELECTED_FEATURES].isnull().sum()\n",
        "missing_total = missing_counts.sum()\n",
        "\n",
        "if missing_total > 0:\n",
        "    print(f\"  [!]  Missing values detected:\")\n",
        "    for feat, count in missing_counts[missing_counts > 0].items():\n",
        "        print(f\"    - {feat}: {count} ({count/len(user_agg)*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\n  Filling with median imputation...\")\n",
        "    for feat in SELECTED_FEATURES:\n",
        "        if user_agg[feat].isnull().any():\n",
        "            median_val = user_agg[feat].median()\n",
        "            user_agg[feat].fillna(median_val, inplace=True)\n",
        "            print(f\"    - {feat}: filled with {median_val:.3f}\")\n",
        "else:\n",
        "    print(f\"  [OK] No missing values\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n[3/3] Feature statistics summary...\")\n",
        "\n",
        "print(\"\\nDescriptive statistics (selected features):\")\n",
        "print(user_agg[SELECTED_FEATURES].describe().T[['mean', 'std', 'min', 'max']].round(2))\n",
        "\n",
        "print()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaWIP594X1T4",
        "outputId": "1016fe6b-e217-46d7-8a11-b788b77a9490"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SECTION 5: FEATURE SELECTION & VALIDATION\n",
            "================================================================================\n",
            "\n",
            "[1/3] Verifying selected features exist...\n",
            "  [OK] All 7 features present\n",
            "\n",
            "[2/3] Checking for missing values...\n",
            "  [OK] No missing values\n",
            "\n",
            "[3/3] Feature statistics summary...\n",
            "\n",
            "Descriptive statistics (selected features):\n",
            "                      mean      std     min       max\n",
            "reps_mean             8.79     0.15    8.55      9.21\n",
            "rpe_mean              4.81     0.31    4.04      5.65\n",
            "total_sets         3072.44  2880.31  125.00  13527.00\n",
            "acwr_mean             1.14     0.06    1.03      1.39\n",
            "spike_weeks_count     8.70     4.80    0.00     25.00\n",
            "load_progression      1.08     0.15    0.95      2.00\n",
            "skip_rate             0.13     0.07    0.03      0.38\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 6 - PREPARE X, y**"
      ],
      "metadata": {
        "id": "6e4GgnzcXlNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 6: PREPARE X, y\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 6: PREPARE X, y\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "X = user_agg[SELECTED_FEATURES].copy()\n",
        "y = user_agg['experience_label'].copy()\n",
        "\n",
        "print(f\"\\n[OK] X shape: {X.shape}\")\n",
        "print(f\"[OK] y shape: {y.shape}\")\n",
        "print(f\"[OK] Feature count: {X.shape[1]}\")\n",
        "\n",
        "# Verify no missing\n",
        "assert X.isnull().sum().sum() == 0, \"X contains missing values!\"\n",
        "assert y.isnull().sum() == 0, \"y contains missing values!\"\n",
        "\n",
        "print(\"[OK] No missing values in X, y\")\n",
        "print()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD_1GryjX5Yi",
        "outputId": "1a345738-a5e9-4bf8-e6ec-40021404e7f6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SECTION 6: PREPARE X, y\n",
            "================================================================================\n",
            "\n",
            "[OK] X shape: (510, 7)\n",
            "[OK] y shape: (510,)\n",
            "[OK] Feature count: 7\n",
            "[OK] No missing values in X, y\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 7 - TRAIN/TEST SPLIT**"
      ],
      "metadata": {
        "id": "Xeyxi0-SXlR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 7: TRAIN/TEST SPLIT (Stratified)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 7: TRAIN/TEST SPLIT (80/20, Stratified)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.20,\n",
        "    random_state=SEED_SPLIT,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\n[OK] Train set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"[OK] Test set:  {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "\n",
        "# Verify stratification\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"STRATIFICATION VERIFICATION\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "train_dist = y_train.value_counts().sort_index()\n",
        "test_dist = y_test.value_counts().sort_index()\n",
        "\n",
        "print(\"\\nTrain distribution:\")\n",
        "for label, count in train_dist.items():\n",
        "    print(f\"  {label:12s}: {count:3d} ({count/len(y_train)*100:5.1f}%)\")\n",
        "\n",
        "print(\"\\nTest distribution:\")\n",
        "for label, count in test_dist.items():\n",
        "    print(f\"  {label:12s}: {count:3d} ({count/len(y_test)*100:5.1f}%)\")\n",
        "\n",
        "# Check balance\n",
        "train_ratios = train_dist / len(y_train)\n",
        "test_ratios = test_dist / len(y_test)\n",
        "max_diff = (train_ratios - test_ratios).abs().max()\n",
        "\n",
        "print(f\"\\n[OK] Max stratification difference: {max_diff:.3%}\")\n",
        "if max_diff < 0.02:\n",
        "    print(\"  [OK] Excellent stratification (< 2%)\")\n",
        "elif max_diff < 0.05:\n",
        "    print(\"  [OK] Good stratification (< 5%)\")\n",
        "else:\n",
        "    print(\"  [!]  Moderate stratification (> 5%)\")\n",
        "\n",
        "print()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FltTGcyGX7no",
        "outputId": "b7a36fc3-f06a-4088-bfc0-525cce378409"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SECTION 7: TRAIN/TEST SPLIT (80/20, Stratified)\n",
            "================================================================================\n",
            "\n",
            "[OK] Train set: 408 samples (80.0%)\n",
            "[OK] Test set:  102 samples (20.0%)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STRATIFICATION VERIFICATION\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Train distribution:\n",
            "  Advanced    : 136 ( 33.3%)\n",
            "  Beginner    : 136 ( 33.3%)\n",
            "  Intermediate: 136 ( 33.3%)\n",
            "\n",
            "Test distribution:\n",
            "  Advanced    :  34 ( 33.3%)\n",
            "  Beginner    :  34 ( 33.3%)\n",
            "  Intermediate:  34 ( 33.3%)\n",
            "\n",
            "[OK] Max stratification difference: 0.000%\n",
            "  [OK] Excellent stratification (< 2%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 8 - FEATURE SCALING**"
      ],
      "metadata": {
        "id": "JiMOPFwcXlWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 8: FEATURE SCALING (StandardScaler)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 8: FEATURE SCALING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit on train, transform both\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrame (preserves feature names)\n",
        "X_train_scaled = pd.DataFrame(\n",
        "    X_train_scaled,\n",
        "    columns=SELECTED_FEATURES,\n",
        "    index=X_train.index\n",
        ")\n",
        "\n",
        "X_test_scaled = pd.DataFrame(\n",
        "    X_test_scaled,\n",
        "    columns=SELECTED_FEATURES,\n",
        "    index=X_test.index\n",
        ")\n",
        "\n",
        "print(f\"\\n[OK] Scaler fitted on train set ({len(X_train)} samples)\")\n",
        "print(f\"[OK] Train and test transformed\")\n",
        "\n",
        "# Verify scaling (mean≈0, std≈1 on train)\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"SCALING VERIFICATION (Train Set)\")\n",
        "print(\"-\"*80)\n",
        "print(\"\\nMean (should be ≈0):\")\n",
        "print(X_train_scaled.mean().round(6))\n",
        "print(\"\\nStd (should be ≈1):\")\n",
        "print(X_train_scaled.std().round(6))\n",
        "\n",
        "# Check test set (mean/std will differ, expected)\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"TEST SET STATISTICS (After Scaling)\")\n",
        "print(\"-\"*80)\n",
        "print(\"\\nMean (may differ from 0):\")\n",
        "print(X_test_scaled.mean().round(3))\n",
        "print(\"\\nStd (may differ from 1):\")\n",
        "print(X_test_scaled.std().round(3))\n",
        "\n",
        "print(\"\\n[OK] Scaling complete\")\n",
        "print()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plEsumlDX-Ir",
        "outputId": "346a315e-67c9-42c1-b47c-5a7bb6843022"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SECTION 8: FEATURE SCALING\n",
            "================================================================================\n",
            "\n",
            "[OK] Scaler fitted on train set (408 samples)\n",
            "[OK] Train and test transformed\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "SCALING VERIFICATION (Train Set)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Mean (should be ≈0):\n",
            "reps_mean           -0.0\n",
            "rpe_mean             0.0\n",
            "total_sets          -0.0\n",
            "acwr_mean           -0.0\n",
            "spike_weeks_count    0.0\n",
            "load_progression     0.0\n",
            "skip_rate            0.0\n",
            "dtype: float64\n",
            "\n",
            "Std (should be ≈1):\n",
            "reps_mean            1.001228\n",
            "rpe_mean             1.001228\n",
            "total_sets           1.001228\n",
            "acwr_mean            1.001228\n",
            "spike_weeks_count    1.001228\n",
            "load_progression     1.001228\n",
            "skip_rate            1.001228\n",
            "dtype: float64\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TEST SET STATISTICS (After Scaling)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Mean (may differ from 0):\n",
            "reps_mean            0.006\n",
            "rpe_mean             0.127\n",
            "total_sets          -0.050\n",
            "acwr_mean            0.083\n",
            "spike_weeks_count    0.114\n",
            "load_progression     0.010\n",
            "skip_rate            0.008\n",
            "dtype: float64\n",
            "\n",
            "Std (may differ from 1):\n",
            "reps_mean            1.020\n",
            "rpe_mean             1.010\n",
            "total_sets           0.917\n",
            "acwr_mean            1.054\n",
            "spike_weeks_count    1.066\n",
            "load_progression     0.885\n",
            "skip_rate            1.065\n",
            "dtype: float64\n",
            "\n",
            "[OK] Scaling complete\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 9 - SAVE PREPROCESSED DATA**"
      ],
      "metadata": {
        "id": "uDiKB52mXlaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 9: SAVE PREPROCESSED DATA\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 9: SAVE PREPROCESSED DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Prepare data bundle\n",
        "preprocessed_data = {\n",
        "    'X_train': X_train_scaled,\n",
        "    'X_test': X_test_scaled,\n",
        "    'y_train': y_train,\n",
        "    'y_test': y_test,\n",
        "    'scaler': scaler,\n",
        "    'feature_names': SELECTED_FEATURES,\n",
        "    'target_encoding': TARGET_ENCODING,\n",
        "    'seed_split': SEED_SPLIT,\n",
        "    'version': '2.2',\n",
        "    'timestamp': datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "# Save as pickle\n",
        "output_path = MODELDIR / 'status_preprocessed_v2.2.pkl'\n",
        "\n",
        "with open(output_path, 'wb') as f:\n",
        "    pickle.dump(preprocessed_data, f)\n",
        "\n",
        "print(f\"\\n[OK] Preprocessed data saved: {output_path}\")\n",
        "print(f\"  File size: {output_path.stat().st_size / 1024:.1f} KB\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"SAVE FEATURE METADATA (for interpretability)\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Feature metadata\n",
        "feature_metadata = {\n",
        "    'version': '2.2',\n",
        "    'date_created': datetime.now().isoformat(),\n",
        "    'n_features': len(SELECTED_FEATURES),\n",
        "    'features': [],\n",
        "    'removed_features': [\n",
        "        {\n",
        "            'name': 'consistency_score',\n",
        "            'reason': 'Perfect leakage (correlation 1.000, deterministic)',\n",
        "            'cohens_d': 'infinite',\n",
        "            'correlation': 1.000\n",
        "        },\n",
        "        {\n",
        "            'name': 'load_mean',\n",
        "            'reason': 'Borderline leakage risk (correlation 0.803)',\n",
        "            'cohens_d': 2.94,\n",
        "            'correlation': 0.803\n",
        "        },\n",
        "        {\n",
        "            'name': 'observed_freq_weekly',\n",
        "            'reason': 'Low discriminative power (Cohen\\'s d 0.04)',\n",
        "            'cohens_d': 0.04,\n",
        "            'correlation': 0.016\n",
        "        }\n",
        "    ],\n",
        "    'train_test_split': {\n",
        "        'test_size': 0.20,\n",
        "        'random_state': SEED_SPLIT,\n",
        "        'stratified': True,\n",
        "        'n_train': len(X_train),\n",
        "        'n_test': len(X_test)\n",
        "    },\n",
        "    'scaling': {\n",
        "        'method': 'StandardScaler',\n",
        "        'fit_on': 'train_set'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Add feature info (from EDA results, manually entered)\n",
        "feature_info = [\n",
        "    {'name': 'reps_mean', 'cohens_d': 3.30, 'correlation': -0.761, 'interpretation': 'Average reps per set (Beginner higher)'},\n",
        "    {'name': 'rpe_mean', 'cohens_d': 2.21, 'correlation': -0.673, 'interpretation': 'RPE self-report (Beginner overestimate)'},\n",
        "    {'name': 'total_sets', 'cohens_d': 2.61, 'correlation': 0.755, 'interpretation': 'Training history length'},\n",
        "    {'name': 'acwr_mean', 'cohens_d': 2.16, 'correlation': -0.691, 'interpretation': 'Load management (Beginner higher ACWR)'},\n",
        "    {'name': 'spike_weeks_count', 'cohens_d': 0.650, 'correlation': 0.247, 'interpretation': 'Frequency of overload weeks'},\n",
        "    {'name': 'load_progression', 'cohens_d': 1.00, 'correlation': 0.432, 'interpretation': 'Monthly load growth rate'},\n",
        "    {'name': 'skip_rate', 'cohens_d': 3.66, 'correlation': -0.839, 'interpretation': 'Workout dropout rate (fatigue-driven)'}\n",
        "]\n",
        "\n",
        "feature_metadata['features'] = feature_info\n",
        "\n",
        "# Save metadata as JSON\n",
        "metadata_path = MODELDIR / 'status_feature_metadata_v2.2.json'\n",
        "\n",
        "with open(metadata_path, 'w') as f:\n",
        "    json.dump(feature_metadata, f, indent=2)\n",
        "\n",
        "print(f\"\\n[OK] Feature metadata saved: {metadata_path}\")\n",
        "print(f\"  File size: {metadata_path.stat().st_size / 1024:.1f} KB\")\n",
        "\n",
        "print()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmxWa9CGYAZw",
        "outputId": "4769ac6c-ad8c-489f-d3e3-9f0c5780c462"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SECTION 9: SAVE PREPROCESSED DATA\n",
            "================================================================================\n",
            "\n",
            "[OK] Preprocessed data saved: models/status_preprocessed_v2.2.pkl\n",
            "  File size: 43.1 KB\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "SAVE FEATURE METADATA (for interpretability)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[OK] Feature metadata saved: models/status_feature_metadata_v2.2.json\n",
            "  File size: 1.9 KB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 10 - SUMMARY & NEXT STEPS**"
      ],
      "metadata": {
        "id": "xzKp8xkVXleQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 10: SUMMARY & NEXT STEPS\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"FEATURE ENGINEERING v2.2 - SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n\" + \"┌\" + \"─\"*78 + \"┐\")\n",
        "print(\"│\" + \" \"*28 + \"PREPROCESSING COMPLETE\" + \" \"*28 + \"│\")\n",
        "print(\"└\" + \"─\"*78 + \"┘\")\n",
        "\n",
        "print(\"\\nDATASET STATISTICS\")\n",
        "print(\"-\"*80)\n",
        "print(f\"Total users:          {len(user_agg):,}\")\n",
        "print(f\"Train set:            {len(X_train):,} ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"Test set:             {len(X_test):,} ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "print(f\"Features selected:    {len(SELECTED_FEATURES)}\")\n",
        "\n",
        "print(\"\\nTARGET DISTRIBUTION\")\n",
        "print(\"-\"*80)\n",
        "print(\"Train:\")\n",
        "for label, count in y_train.value_counts().sort_index().items():\n",
        "    print(f\"  {label:12s}: {count:3d} ({count/len(y_train)*100:5.1f}%)\")\n",
        "print(\"Test:\")\n",
        "for label, count in y_test.value_counts().sort_index().items():\n",
        "    print(f\"  {label:12s}: {count:3d} ({count/len(y_test)*100:5.1f}%)\")\n",
        "\n",
        "print(\"\\nFEATURE ENGINEERING CHOICES\")\n",
        "print(\"-\"*80)\n",
        "print(\"Selected features (7):\")\n",
        "for i, feat in enumerate(SELECTED_FEATURES, 1):\n",
        "    print(f\"  {i}. {feat}\")\n",
        "\n",
        "print(\"\\nRemoved features (3):\")\n",
        "print(\"  1. consistency_score (leakage: correlation 1.000)\")\n",
        "print(\"  2. load_mean (leakage risk: correlation 0.803)\")\n",
        "print(\"  3. observed_freq_weekly (low discriminative power)\")\n",
        "\n",
        "print(\"\\nOUTPUT FILES\")\n",
        "print(\"-\"*80)\n",
        "print(f\"[OK] {output_path}\")\n",
        "print(f\"[OK] {metadata_path}\")\n",
        "\n",
        "print(\"\\n NEXT STEPS\")\n",
        "print(\"-\"*80)\n",
        "print(\"1. Open STATUS_Modeling_v2.2.ipynb\")\n",
        "print(\"2. Load preprocessed data:\")\n",
        "print(f\"   with open('{output_path}', 'rb') as f:\")\n",
        "print(\"       data = pickle.load(f)\")\n",
        "print(\"3. Train 6 models (Dummy, LR, DT, RF, GB, XGBoost)\")\n",
        "print(\"4. Evaluate on tests sets \")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEATURE ENGINEERING v2.2 COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SujorqT_YBxa",
        "outputId": "33abcb23-2418-4d8c-cb4d-da2a8017c7e6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "FEATURE ENGINEERING v2.2 - SUMMARY\n",
            "================================================================================\n",
            "\n",
            "┌──────────────────────────────────────────────────────────────────────────────┐\n",
            "│                            PREPROCESSING COMPLETE                            │\n",
            "└──────────────────────────────────────────────────────────────────────────────┘\n",
            "\n",
            "DATASET STATISTICS\n",
            "--------------------------------------------------------------------------------\n",
            "Total users:          510\n",
            "Train set:            408 (80.0%)\n",
            "Test set:             102 (20.0%)\n",
            "Features selected:    7\n",
            "\n",
            "TARGET DISTRIBUTION\n",
            "--------------------------------------------------------------------------------\n",
            "Train:\n",
            "  Advanced    : 136 ( 33.3%)\n",
            "  Beginner    : 136 ( 33.3%)\n",
            "  Intermediate: 136 ( 33.3%)\n",
            "Test:\n",
            "  Advanced    :  34 ( 33.3%)\n",
            "  Beginner    :  34 ( 33.3%)\n",
            "  Intermediate:  34 ( 33.3%)\n",
            "\n",
            "FEATURE ENGINEERING CHOICES\n",
            "--------------------------------------------------------------------------------\n",
            "Selected features (7):\n",
            "  1. reps_mean\n",
            "  2. rpe_mean\n",
            "  3. total_sets\n",
            "  4. acwr_mean\n",
            "  5. spike_weeks_count\n",
            "  6. load_progression\n",
            "  7. skip_rate\n",
            "\n",
            "Removed features (3):\n",
            "  1. consistency_score (leakage: correlation 1.000)\n",
            "  2. load_mean (leakage risk: correlation 0.803)\n",
            "  3. observed_freq_weekly (low discriminative power)\n",
            "\n",
            "OUTPUT FILES\n",
            "--------------------------------------------------------------------------------\n",
            "[OK] models/status_preprocessed_v2.2.pkl\n",
            "[OK] models/status_feature_metadata_v2.2.json\n",
            "\n",
            " NEXT STEPS\n",
            "--------------------------------------------------------------------------------\n",
            "1. Open STATUS_Modeling_v2.2.ipynb\n",
            "2. Load preprocessed data:\n",
            "   with open('models/status_preprocessed_v2.2.pkl', 'rb') as f:\n",
            "       data = pickle.load(f)\n",
            "3. Train 6 models (Dummy, LR, DT, RF, GB, XGBoost)\n",
            "4. Evaluate on tests sets \n",
            "\n",
            "================================================================================\n",
            "FEATURE ENGINEERING v2.2 COMPLETE\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}