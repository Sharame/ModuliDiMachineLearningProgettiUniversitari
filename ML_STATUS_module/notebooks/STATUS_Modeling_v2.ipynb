{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhbIAudJOwWF0XqJ41R5Qa"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 1 - CONFIG AND LOAD DATA**"
      ],
      "metadata": {
        "id": "UZ7XKGfi0d3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# STATUS MODULE - MODELING v2.2 (Model Comparison & Selection)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "#\n",
        "# OBIETTIVO:\n",
        "# Comparare 6 modelli per classificazione experience level, selezionare migliore\n",
        "#\n",
        "# INPUT:\n",
        "#   - models/status_preprocessed_v2.2.pkl (X_train/test, y_train/test, scaler)\n",
        "#\n",
        "# OUTPUT:\n",
        "#   - models/status_best_model_v2.2.pkl (best model trained)\n",
        "#   - models/status_model_comparison_v2.2.json (performance metrics)\n",
        "#   - visualizations/STATUS_Modeling_v2.2/ (confusion matrices, feature importance)\n",
        "#\n",
        "# MODELLI TESTATI:\n",
        "#   1. Dummy Classifier (baseline assoluto)\n",
        "#   2. Logistic Regression (linear baseline)\n",
        "#   3. Decision Tree (interpretable, single tree)\n",
        "#   4. Random Forest (bagging ensemble)\n",
        "#   5. Gradient Boosting (sklearn boosting baseline)\n",
        "#   6. XGBoost (candidate finale, regularized)\n",
        "#\n",
        "# METRICHE:\n",
        "#   - Accuracy (test set)\n",
        "#   - F1-macro, F1 per-class\n",
        "#   - Train-test gap (overfitting check)\n",
        "#   - Training time\n",
        "#   - Feature importance (tree-based models)\n",
        "#\n",
        "# VERSIONE: 2.2\n",
        "# DATA: 2026-02-09\n",
        "# AUTORE: Alessandro Ambrosio\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, classification_report,\n",
        "    confusion_matrix, ConfusionMatrixDisplay\n",
        ")\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "\n",
        "# XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STATUS MODULE - MODELING v2.2 (Model Comparison)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PIuU7TU0ihd",
        "outputId": "1c8cc7c3-892e-4d89-a5a5-e399282046a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STATUS MODULE - MODELING v2.2 (Model Comparison)\n",
            "================================================================================\n",
            "Timestamp: 2026-02-09 10:35:43\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 1: LOAD PREPROCESSED DATA\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 1: LOAD PREPROCESSED DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Paths\n",
        "MODELDIR = Path('models')\n",
        "VIZDIR = Path('visualizations/STATUS_Modeling_v2.2')\n",
        "VIZDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load preprocessed data\n",
        "preprocessed_path = MODELDIR / 'status_preprocessed_v2.2.pkl'\n",
        "\n",
        "print(f\"\\n[OK] Loading: {preprocessed_path}\")\n",
        "\n",
        "with open(preprocessed_path, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "X_train = data['X_train']\n",
        "X_test = data['X_test']\n",
        "y_train = data['y_train']\n",
        "y_test = data['y_test']\n",
        "scaler = data['scaler']\n",
        "feature_names = data['feature_names']\n",
        "target_encoding = data['target_encoding']\n",
        "\n",
        "print(f\"\\n[OK] X_train shape: {X_train.shape}\")\n",
        "print(f\"[OK] X_test shape: {X_test.shape}\")\n",
        "print(f\"[OK] Features: {len(feature_names)}\")\n",
        "print(f\"[OK] Features: {feature_names}\")\n",
        "\n",
        "# Target distribution\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"TARGET DISTRIBUTION\")\n",
        "print(\"-\"*80)\n",
        "print(\"\\nTrain:\")\n",
        "print(y_train.value_counts().sort_index())\n",
        "print(\"\\nTest:\")\n",
        "print(y_test.value_counts().sort_index())\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Teuh5nrD0-HK",
        "outputId": "fdd35ca5-cf44-4fe6-c301-efc9967a336d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SECTION 1: LOAD PREPROCESSED DATA\n",
            "================================================================================\n",
            "\n",
            "[OK] Loading: models/status_preprocessed_v2.2.pkl\n",
            "\n",
            "[OK] X_train shape: (408, 7)\n",
            "[OK] X_test shape: (102, 7)\n",
            "[OK] Features: 7\n",
            "[OK] Features: ['reps_mean', 'rpe_mean', 'total_sets', 'acwr_mean', 'spike_weeks_count', 'load_progression', 'skip_rate']\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TARGET DISTRIBUTION\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Train:\n",
            "experience_label\n",
            "Advanced        136\n",
            "Beginner        136\n",
            "Intermediate    136\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Test:\n",
            "experience_label\n",
            "Advanced        34\n",
            "Beginner        34\n",
            "Intermediate    34\n",
            "Name: count, dtype: int64\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 3 - BASELINE MODELS**"
      ],
      "metadata": {
        "id": "bxvQQwS30hjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 2: BASELINE MODELS\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 2: BASELINE MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Storage for results\n",
        "results = []\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# MODEL 1: Dummy Classifier (Stratified)\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n[1/6] Dummy Classifier (Stratified Baseline)...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "dummy = DummyClassifier(strategy='stratified', random_state=42)\n",
        "dummy.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = dummy.predict(X_train)\n",
        "y_test_pred = dummy.predict(X_test)\n",
        "\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
        "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "print(f\"  Train Accuracy: {train_acc:.3f}\")\n",
        "print(f\"  Test Accuracy:  {test_acc:.3f}\")\n",
        "print(f\"  Test F1-macro:  {test_f1:.3f}\")\n",
        "print(f\"  Time: {elapsed:.2f}s\")\n",
        "\n",
        "results.append({\n",
        "    'model': 'Dummy (Stratified)',\n",
        "    'train_acc': train_acc,\n",
        "    'test_acc': test_acc,\n",
        "    'train_f1': train_f1,\n",
        "    'test_f1': test_f1,\n",
        "    'gap': train_acc - test_acc,\n",
        "    'time_sec': elapsed\n",
        "})\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# MODEL 2: Logistic Regression\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n[2/6] Logistic Regression (Linear Baseline)...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "logreg = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    random_state=42,\n",
        "    solver='lbfgs',\n",
        "    multi_class='multinomial'\n",
        ")\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = logreg.predict(X_train)\n",
        "y_test_pred = logreg.predict(X_test)\n",
        "\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
        "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "print(f\"  Train Accuracy: {train_acc:.3f}\")\n",
        "print(f\"  Test Accuracy:  {test_acc:.3f}\")\n",
        "print(f\"  Test F1-macro:  {test_f1:.3f}\")\n",
        "print(f\"  Gap: {train_acc - test_acc:.3f}\")\n",
        "print(f\"  Time: {elapsed:.2f}s\")\n",
        "\n",
        "results.append({\n",
        "    'model': 'Logistic Regression',\n",
        "    'train_acc': train_acc,\n",
        "    'test_acc': test_acc,\n",
        "    'train_f1': train_f1,\n",
        "    'test_f1': test_f1,\n",
        "    'gap': train_acc - test_acc,\n",
        "    'time_sec': elapsed\n",
        "})\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# MODEL 3: Decision Tree\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n[3/6] Decision Tree (Interpretable Baseline)...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "dt = DecisionTreeClassifier(\n",
        "    max_depth=5,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=5,\n",
        "    random_state=42\n",
        ")\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = dt.predict(X_train)\n",
        "y_test_pred = dt.predict(X_test)\n",
        "\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
        "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "print(f\"  Train Accuracy: {train_acc:.3f}\")\n",
        "print(f\"  Test Accuracy:  {test_acc:.3f}\")\n",
        "print(f\"  Test F1-macro:  {test_f1:.3f}\")\n",
        "print(f\"  Gap: {train_acc - test_acc:.3f}\")\n",
        "print(f\"  Time: {elapsed:.2f}s\")\n",
        "\n",
        "results.append({\n",
        "    'model': 'Decision Tree',\n",
        "    'train_acc': train_acc,\n",
        "    'test_acc': test_acc,\n",
        "    'train_f1': train_f1,\n",
        "    'test_f1': test_f1,\n",
        "    'gap': train_acc - test_acc,\n",
        "    'time_sec': elapsed\n",
        "})\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RTOBK9s0jcD",
        "outputId": "23458fce-bb2e-4a72-e327-87372dd60eb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SECTION 2: BASELINE MODELS\n",
            "================================================================================\n",
            "\n",
            "[1/6] Dummy Classifier (Stratified Baseline)...\n",
            "  Train Accuracy: 0.368\n",
            "  Test Accuracy:  0.294\n",
            "  Test F1-macro:  0.295\n",
            "  Time: 0.02s\n",
            "\n",
            "[2/6] Logistic Regression (Linear Baseline)...\n",
            "  Train Accuracy: 0.968\n",
            "  Test Accuracy:  0.941\n",
            "  Test F1-macro:  0.941\n",
            "  Gap: 0.027\n",
            "  Time: 0.11s\n",
            "\n",
            "[3/6] Decision Tree (Interpretable Baseline)...\n",
            "  Train Accuracy: 0.944\n",
            "  Test Accuracy:  0.912\n",
            "  Test F1-macro:  0.912\n",
            "  Gap: 0.032\n",
            "  Time: 0.06s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 4 - ENSEMBLE METHODS**"
      ],
      "metadata": {
        "id": "_GhGcnEO0hlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 3: ENSEMBLE METHODS\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 3: ENSEMBLE METHODS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# MODEL 4: Random Forest\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n[4/6] Random Forest (Bagging Ensemble)...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=8,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=4,\n",
        "    max_features='sqrt',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = rf.predict(X_train)\n",
        "y_test_pred = rf.predict(X_test)\n",
        "\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
        "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "print(f\"  Train Accuracy: {train_acc:.3f}\")\n",
        "print(f\"  Test Accuracy:  {test_acc:.3f}\")\n",
        "print(f\"  Test F1-macro:  {test_f1:.3f}\")\n",
        "print(f\"  Gap: {train_acc - test_acc:.3f}\")\n",
        "print(f\"  Time: {elapsed:.2f}s\")\n",
        "\n",
        "results.append({\n",
        "    'model': 'Random Forest',\n",
        "    'train_acc': train_acc,\n",
        "    'test_acc': test_acc,\n",
        "    'train_f1': train_f1,\n",
        "    'test_f1': test_f1,\n",
        "    'gap': train_acc - test_acc,\n",
        "    'time_sec': elapsed\n",
        "})\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# MODEL 5: Gradient Boosting (sklearn)\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n[5/6] Gradient Boosting (sklearn baseline)...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "gb = GradientBoostingClassifier(\n",
        "    n_estimators=150,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=4,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=4,\n",
        "    subsample=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = gb.predict(X_train)\n",
        "y_test_pred = gb.predict(X_test)\n",
        "\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
        "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "print(f\"  Train Accuracy: {train_acc:.3f}\")\n",
        "print(f\"  Test Accuracy:  {test_acc:.3f}\")\n",
        "print(f\"  Test F1-macro:  {test_f1:.3f}\")\n",
        "print(f\"  Gap: {train_acc - test_acc:.3f}\")\n",
        "print(f\"  Time: {elapsed:.2f}s\")\n",
        "\n",
        "results.append({\n",
        "    'model': 'Gradient Boosting',\n",
        "    'train_acc': train_acc,\n",
        "    'test_acc': test_acc,\n",
        "    'train_f1': train_f1,\n",
        "    'test_f1': test_f1,\n",
        "    'gap': train_acc - test_acc,\n",
        "    'time_sec': elapsed\n",
        "})\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# MODEL 6: XGBoost (Candidate Finale)\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n[6/6] XGBoost (Regularized, Candidate Finale)...\")\n",
        "\n",
        "# XGBoost requires numeric labels\n",
        "label_encoder = {'Beginner': 0, 'Intermediate': 1, 'Advanced': 2}\n",
        "y_train_encoded = y_train.map(label_encoder)\n",
        "y_test_encoded = y_test.map(label_encoder)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=150,\n",
        "    max_depth=4,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.1,          # L1 regularization\n",
        "    reg_lambda=1.0,         # L2 regularization\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    eval_metric='mlogloss'\n",
        ")\n",
        "xgb.fit(X_train, y_train_encoded)\n",
        "\n",
        "y_train_pred_encoded = xgb.predict(X_train)\n",
        "y_test_pred_encoded = xgb.predict(X_test)\n",
        "\n",
        "# Decode back to string labels for consistency\n",
        "reverse_encoder = {0: 'Beginner', 1: 'Intermediate', 2: 'Advanced'}\n",
        "y_train_pred = pd.Series(y_train_pred_encoded).map(reverse_encoder)\n",
        "y_test_pred = pd.Series(y_test_pred_encoded).map(reverse_encoder)\n",
        "\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
        "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "print(f\"  Train Accuracy: {train_acc:.3f}\")\n",
        "print(f\"  Test Accuracy:  {test_acc:.3f}\")\n",
        "print(f\"  Test F1-macro:  {test_f1:.3f}\")\n",
        "print(f\"  Gap: {train_acc - test_acc:.3f}\")\n",
        "print(f\"  Time: {elapsed:.2f}s\")\n",
        "\n",
        "results.append({\n",
        "    'model': 'XGBoost',\n",
        "    'train_acc': train_acc,\n",
        "    'test_acc': test_acc,\n",
        "    'train_f1': train_f1,\n",
        "    'test_f1': test_f1,\n",
        "    'gap': train_acc - test_acc,\n",
        "    'time_sec': elapsed\n",
        "})\n",
        "\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTDTUVzJ0j3l",
        "outputId": "08f719b3-81b7-4e67-f6f7-73469c012e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SECTION 3: ENSEMBLE METHODS\n",
            "================================================================================\n",
            "\n",
            "[4/6] Random Forest (Bagging Ensemble)...\n",
            "  Train Accuracy: 0.985\n",
            "  Test Accuracy:  0.941\n",
            "  Test F1-macro:  0.941\n",
            "  Gap: 0.044\n",
            "  Time: 0.85s\n",
            "\n",
            "[5/6] Gradient Boosting (sklearn baseline)...\n",
            "  Train Accuracy: 1.000\n",
            "  Test Accuracy:  0.961\n",
            "  Test F1-macro:  0.961\n",
            "  Gap: 0.039\n",
            "  Time: 1.75s\n",
            "\n",
            "[6/6] XGBoost (Regularized, Candidate Finale)...\n",
            "  Train Accuracy: 1.000\n",
            "  Test Accuracy:  0.931\n",
            "  Test F1-macro:  0.931\n",
            "  Gap: 0.069\n",
            "  Time: 0.20s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 5 - MODEL COMPARISON (Extended Metrics)**"
      ],
      "metadata": {
        "id": "SrOkx_sl0hqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 4: MODEL COMPARISON (Extended Metrics)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 4: MODEL COMPARISON (Extended Metrics)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "# Re-train all models and collect extended metrics\n",
        "models_dict = {\n",
        "    'Dummy (Stratified)': DummyClassifier(strategy='stratified', random_state=42),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, solver='lbfgs', multi_class='multinomial'),\n",
        "    'Decision Tree': DecisionTreeClassifier(max_depth=5, min_samples_split=10, min_samples_leaf=5, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=8, min_samples_split=10, min_samples_leaf=4, max_features='sqrt', random_state=42, n_jobs=-1),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=150, learning_rate=0.1, max_depth=4, min_samples_split=10, min_samples_leaf=4, subsample=0.8, random_state=42),\n",
        "    'XGBoost': XGBClassifier(n_estimators=150, max_depth=4, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=1.0, random_state=42, n_jobs=-1, eval_metric='mlogloss')\n",
        "}\n",
        "\n",
        "# Extended results storage\n",
        "extended_results = []\n",
        "\n",
        "print(\"\\nTraining models with extended metrics...\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for model_name, model in models_dict.items():\n",
        "    print(f\"\\n{model_name}...\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Handle XGBoost encoding\n",
        "    if model_name == 'XGBoost':\n",
        "        label_encoder = {'Beginner': 0, 'Intermediate': 1, 'Advanced': 2}\n",
        "        y_train_encoded = y_train.map(label_encoder)\n",
        "        y_test_encoded = y_test.map(label_encoder)\n",
        "\n",
        "        model.fit(X_train, y_train_encoded)\n",
        "\n",
        "        y_train_pred_encoded = model.predict(X_train)\n",
        "        y_test_pred_encoded = model.predict(X_test)\n",
        "\n",
        "        reverse_encoder = {0: 'Beginner', 1: 'Intermediate', 2: 'Advanced'}\n",
        "        y_train_pred = pd.Series(y_train_pred_encoded).map(reverse_encoder).values\n",
        "        y_test_pred = pd.Series(y_test_pred_encoded).map(reverse_encoder).values\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        y_test_pred = model.predict(X_test)\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    # Calculate metrics\n",
        "    train_acc = accuracy_score(y_train, y_train_pred)\n",
        "    test_acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
        "    test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "    train_precision = precision_score(y_train, y_train_pred, average='macro')\n",
        "    test_precision = precision_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "    train_recall = recall_score(y_train, y_train_pred, average='macro')\n",
        "    test_recall = recall_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "    # Per-class metrics (test set only)\n",
        "    test_precision_per_class = precision_score(y_test, y_test_pred, average=None, labels=['Beginner', 'Intermediate', 'Advanced'])\n",
        "    test_recall_per_class = recall_score(y_test, y_test_pred, average=None, labels=['Beginner', 'Intermediate', 'Advanced'])\n",
        "    test_f1_per_class = f1_score(y_test, y_test_pred, average=None, labels=['Beginner', 'Intermediate', 'Advanced'])\n",
        "\n",
        "    print(f\"  Test Accuracy:  {test_acc:.3f}\")\n",
        "    print(f\"  Test F1-macro:  {test_f1:.3f}\")\n",
        "    print(f\"  Test Precision: {test_precision:.3f}\")\n",
        "    print(f\"  Test Recall:    {test_recall:.3f}\")\n",
        "    print(f\"  Train-Test Gap: {train_acc - test_acc:.3f}\")\n",
        "    print(f\"  Time: {elapsed:.2f}s\")\n",
        "\n",
        "    extended_results.append({\n",
        "        'model': model_name,\n",
        "        'train_acc': train_acc,\n",
        "        'test_acc': test_acc,\n",
        "        'train_f1': train_f1,\n",
        "        'test_f1': test_f1,\n",
        "        'train_precision': train_precision,\n",
        "        'test_precision': test_precision,\n",
        "        'train_recall': train_recall,\n",
        "        'test_recall': test_recall,\n",
        "        'gap': train_acc - test_acc,\n",
        "        'time_sec': elapsed,\n",
        "        'test_precision_beginner': test_precision_per_class[0],\n",
        "        'test_precision_intermediate': test_precision_per_class[1],\n",
        "        'test_precision_advanced': test_precision_per_class[2],\n",
        "        'test_recall_beginner': test_recall_per_class[0],\n",
        "        'test_recall_intermediate': test_recall_per_class[1],\n",
        "        'test_recall_advanced': test_recall_per_class[2],\n",
        "        'test_f1_beginner': test_f1_per_class[0],\n",
        "        'test_f1_intermediate': test_f1_per_class[1],\n",
        "        'test_f1_advanced': test_f1_per_class[2]\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_results = pd.DataFrame(extended_results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON TABLE (Test Set)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Display main metrics\n",
        "print(\"\\nOverall Performance:\")\n",
        "print(df_results[['model', 'test_acc', 'test_f1', 'test_precision', 'test_recall', 'gap', 'time_sec']].to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"Per-Class Performance (Test Set)\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Display per-class metrics\n",
        "print(\"\\nPrecision per class:\")\n",
        "print(df_results[['model', 'test_precision_beginner', 'test_precision_intermediate', 'test_precision_advanced']].to_string(index=False))\n",
        "\n",
        "print(\"\\nRecall per class:\")\n",
        "print(df_results[['model', 'test_recall_beginner', 'test_recall_intermediate', 'test_recall_advanced']].to_string(index=False))\n",
        "\n",
        "print(\"\\nF1-Score per class:\")\n",
        "print(df_results[['model', 'test_f1_beginner', 'test_f1_intermediate', 'test_f1_advanced']].to_string(index=False))\n",
        "\n",
        "# Identify best model (by test F1-macro, considering gap < 0.10)\n",
        "df_filtered = df_results[df_results['gap'] < 0.10]  # Filter overfitting models\n",
        "if len(df_filtered) > 0:\n",
        "    best_model_name = df_filtered.loc[df_filtered['test_f1'].idxmax(), 'model']\n",
        "else:\n",
        "    best_model_name = df_results.loc[df_results['test_f1'].idxmax(), 'model']\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"BEST MODEL: {best_model_name}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Selection criteria: Highest F1-macro with train-test gap < 0.10\")\n",
        "\n",
        "# Save comparison results\n",
        "comparison_path = MODELDIR / 'status_model_comparison_v2.2.json'\n",
        "df_results.to_json(comparison_path, orient='records', indent=2)\n",
        "print(f\"\\n[OK] Comparison saved: {comparison_path}\")\n",
        "\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90YnIoce0kLg",
        "outputId": "4abb56fb-2326-4758-bb09-b196e5b55217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SECTION 4: MODEL COMPARISON (Extended Metrics)\n",
            "================================================================================\n",
            "\n",
            "Training models with extended metrics...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Dummy (Stratified)...\n",
            "  Test Accuracy:  0.294\n",
            "  Test F1-macro:  0.295\n",
            "  Test Precision: 0.297\n",
            "  Test Recall:    0.294\n",
            "  Train-Test Gap: 0.074\n",
            "  Time: 0.00s\n",
            "\n",
            "Logistic Regression...\n",
            "  Test Accuracy:  0.941\n",
            "  Test F1-macro:  0.941\n",
            "  Test Precision: 0.941\n",
            "  Test Recall:    0.941\n",
            "  Train-Test Gap: 0.027\n",
            "  Time: 0.01s\n",
            "\n",
            "Decision Tree...\n",
            "  Test Accuracy:  0.912\n",
            "  Test F1-macro:  0.912\n",
            "  Test Precision: 0.914\n",
            "  Test Recall:    0.912\n",
            "  Train-Test Gap: 0.032\n",
            "  Time: 0.01s\n",
            "\n",
            "Random Forest...\n",
            "  Test Accuracy:  0.941\n",
            "  Test F1-macro:  0.941\n",
            "  Test Precision: 0.942\n",
            "  Test Recall:    0.941\n",
            "  Train-Test Gap: 0.044\n",
            "  Time: 0.80s\n",
            "\n",
            "Gradient Boosting...\n",
            "  Test Accuracy:  0.961\n",
            "  Test F1-macro:  0.961\n",
            "  Test Precision: 0.963\n",
            "  Test Recall:    0.961\n",
            "  Train-Test Gap: 0.039\n",
            "  Time: 2.01s\n",
            "\n",
            "XGBoost...\n",
            "  Test Accuracy:  0.931\n",
            "  Test F1-macro:  0.931\n",
            "  Test Precision: 0.933\n",
            "  Test Recall:    0.931\n",
            "  Train-Test Gap: 0.069\n",
            "  Time: 0.18s\n",
            "\n",
            "================================================================================\n",
            "MODEL COMPARISON TABLE (Test Set)\n",
            "================================================================================\n",
            "\n",
            "Overall Performance:\n",
            "              model  test_acc  test_f1  test_precision  test_recall      gap  time_sec\n",
            " Dummy (Stratified)  0.294118 0.294965        0.296908     0.294118 0.073529  0.004265\n",
            "Logistic Regression  0.941176 0.940882        0.940862     0.941176 0.026961  0.012268\n",
            "      Decision Tree  0.911765 0.911833        0.913850     0.911765 0.031863  0.009813\n",
            "      Random Forest  0.941176 0.940862        0.941653     0.941176 0.044118  0.796591\n",
            "  Gradient Boosting  0.960784 0.960737        0.962698     0.960784 0.039216  2.007156\n",
            "            XGBoost  0.931373 0.931019        0.932634     0.931373 0.068627  0.179804\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Per-Class Performance (Test Set)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Precision per class:\n",
            "              model  test_precision_beginner  test_precision_intermediate  test_precision_advanced\n",
            " Dummy (Stratified)                 0.354839                     0.263158                 0.272727\n",
            "Logistic Regression                 0.971429                     0.911765                 0.939394\n",
            "      Decision Tree                 0.916667                     0.857143                 0.967742\n",
            "      Random Forest                 0.944444                     0.911765                 0.968750\n",
            "  Gradient Boosting                 0.971429                     0.916667                 1.000000\n",
            "            XGBoost                 0.944444                     0.885714                 0.967742\n",
            "\n",
            "Recall per class:\n",
            "              model  test_recall_beginner  test_recall_intermediate  test_recall_advanced\n",
            " Dummy (Stratified)              0.323529                  0.294118              0.264706\n",
            "Logistic Regression              1.000000                  0.911765              0.911765\n",
            "      Decision Tree              0.970588                  0.882353              0.882353\n",
            "      Random Forest              1.000000                  0.911765              0.911765\n",
            "  Gradient Boosting              1.000000                  0.970588              0.911765\n",
            "            XGBoost              1.000000                  0.911765              0.882353\n",
            "\n",
            "F1-Score per class:\n",
            "              model  test_f1_beginner  test_f1_intermediate  test_f1_advanced\n",
            " Dummy (Stratified)          0.338462              0.277778          0.268657\n",
            "Logistic Regression          0.985507              0.911765          0.925373\n",
            "      Decision Tree          0.942857              0.869565          0.923077\n",
            "      Random Forest          0.971429              0.911765          0.939394\n",
            "  Gradient Boosting          0.985507              0.942857          0.953846\n",
            "            XGBoost          0.971429              0.898551          0.923077\n",
            "\n",
            "================================================================================\n",
            "BEST MODEL: Gradient Boosting\n",
            "================================================================================\n",
            "Selection criteria: Highest F1-macro with train-test gap < 0.10\n",
            "\n",
            "[OK] Comparison saved: models/status_model_comparison_v2.2.json\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 6 - VISUALIZATIONS**"
      ],
      "metadata": {
        "id": "sJgyDRtJ0hsP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDjcRZqu0ZG5",
        "outputId": "113ca650-41dd-4818-fde8-0fd3067bb975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SECTION 5: VISUALIZATIONS\n",
            "================================================================================\n",
            "\n",
            "Re-training best model (Gradient Boosting) for analysis...\n",
            "\n",
            "[1/3] Confusion Matrix (STATUS Branding)...\n",
            "  [OK] Saved: visualizations/STATUS_Modeling_v2.2/confusion_matrix_best_model_v2.2.png\n",
            "\n",
            "[2/3] Feature Importance...\n",
            "  [OK] Saved: visualizations/STATUS_Modeling_v2.2/feature_importance_best_model_v2.2.png\n",
            "\n",
            "Top 3 Features:\n",
            "  7. skip_rate           : 0.608\n",
            "  1. reps_mean           : 0.144\n",
            "  3. total_sets          : 0.129\n",
            "\n",
            "[3/3] Model Comparison Chart...\n",
            "  [OK] Saved: visualizations/STATUS_Modeling_v2.2/model_comparison_v2.2.png\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 5: VISUALIZATIONS\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 5: VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# STATUS Brand Colors\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "STATUS_COLORS = {\n",
        "    'navy': '#2B4162',\n",
        "    'royal_blue': '#385F8F',\n",
        "    'purple': '#7B5E9D',\n",
        "    'light_purple': '#9B7EBD',\n",
        "    'text_dark': '#1A1A1A',\n",
        "    'text_light': '#FFFFFF'\n",
        "}\n",
        "\n",
        "STATUS_PALETTE = ['#2B4162', '#7B5E9D', '#9B7EBD']  # Beginner, Intermediate, Advanced\n",
        "\n",
        "# Custom colormap (blue-purple gradient)\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "status_cmap = LinearSegmentedColormap.from_list(\n",
        "    'status',\n",
        "    ['#E8EAF6', '#9B7EBD', '#7B5E9D', '#385F8F', '#2B4162']\n",
        ")\n",
        "\n",
        "# Set seaborn style\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_palette(STATUS_PALETTE)\n",
        "\n",
        "# Re-train best model (Gradient Boosting) for visualization\n",
        "print(\"\\nRe-training best model (Gradient Boosting) for analysis...\")\n",
        "\n",
        "gb_best = GradientBoostingClassifier(\n",
        "    n_estimators=150,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=4,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=4,\n",
        "    subsample=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "gb_best.fit(X_train, y_train)\n",
        "y_test_pred = gb_best.predict(X_test)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# 5.1 Confusion Matrix (STATUS Branding, No Grid)\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n[1/3] Confusion Matrix (STATUS Branding)...\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(9, 7))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_test_pred, labels=['Beginner', 'Intermediate', 'Advanced'])\n",
        "\n",
        "# Custom colormap for confusion matrix\n",
        "disp = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=cm,\n",
        "    display_labels=['Beginner', 'Intermediate', 'Advanced']\n",
        ")\n",
        "disp.plot(cmap=status_cmap, ax=ax, values_format='d', colorbar=False)\n",
        "\n",
        "ax.grid(False)\n",
        "\n",
        "# Customize title and labels\n",
        "ax.set_title('STATUS Module - Confusion Matrix\\nGradient Boosting (Test Set)',\n",
        "             fontsize=16, weight='bold', color=STATUS_COLORS['navy'], pad=20)\n",
        "ax.set_xlabel('Predicted Experience Level', fontsize=13, weight='bold', color=STATUS_COLORS['navy'])\n",
        "ax.set_ylabel('True Experience Level', fontsize=13, weight='bold', color=STATUS_COLORS['navy'])\n",
        "\n",
        "# Add accuracy annotation\n",
        "accuracy = np.trace(cm) / np.sum(cm)\n",
        "ax.text(0.5, -0.15, f'Accuracy: {accuracy:.1%}',\n",
        "        transform=ax.transAxes, ha='center', fontsize=12,\n",
        "        weight='bold', color=STATUS_COLORS['purple'])\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "cm_path = VIZDIR / 'confusion_matrix_best_model_v2.2.png'\n",
        "plt.savefig(cm_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
        "print(f\"  [OK] Saved: {cm_path}\")\n",
        "plt.close()\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# 5.2 Feature Importance\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n[2/3] Feature Importance...\")\n",
        "\n",
        "feature_importance = gb_best.feature_importances_\n",
        "feature_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': feature_importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(11, 7))\n",
        "\n",
        "# Gradient colors (most important = darkest)\n",
        "n_features = len(feature_df)\n",
        "colors = [STATUS_COLORS['navy'] if i == 0\n",
        "          else STATUS_COLORS['royal_blue'] if i == 1\n",
        "          else STATUS_COLORS['purple'] if i == 2\n",
        "          else STATUS_COLORS['light_purple']\n",
        "          for i in range(n_features)]\n",
        "\n",
        "bars = ax.barh(feature_df['feature'], feature_df['importance'], color=colors, edgecolor='white', linewidth=1.5)\n",
        "\n",
        "ax.set_xlabel('Importance Score', fontsize=13, weight='bold', color=STATUS_COLORS['navy'])\n",
        "ax.set_title('STATUS Module - Feature Importance\\nGradient Boosting Classifier',\n",
        "             fontsize=16, weight='bold', color=STATUS_COLORS['navy'], pad=20)\n",
        "ax.invert_yaxis()\n",
        "ax.set_xlim([0, max(feature_df['importance']) * 1.15])\n",
        "\n",
        "# Add value labels\n",
        "for i, (bar, val) in enumerate(zip(bars, feature_df['importance'])):\n",
        "    width = bar.get_width()\n",
        "    ax.text(width + 0.01, bar.get_y() + bar.get_height()/2,\n",
        "            f'{val:.3f}',\n",
        "            ha='left', va='center', fontsize=11, weight='bold',\n",
        "            color=STATUS_COLORS['navy'])\n",
        "\n",
        "# Add ranking numbers\n",
        "for i, bar in enumerate(bars):\n",
        "    ax.text(0.685, bar.get_y() + bar.get_height()/2,\n",
        "            f'#{i+1}',\n",
        "            ha='right', va='center', fontsize=10, weight='bold',\n",
        "            color=STATUS_COLORS['purple'])\n",
        "\n",
        "# Grid styling\n",
        "ax.grid(axis='x', alpha=0.3, linestyle='--', color=STATUS_COLORS['royal_blue'])\n",
        "ax.set_axisbelow(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "fi_path = VIZDIR / 'feature_importance_best_model_v2.2.png'\n",
        "plt.savefig(fi_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
        "print(f\"  [OK] Saved: {fi_path}\")\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nTop 3 Features:\")\n",
        "for i, row in feature_df.head(3).iterrows():\n",
        "    print(f\"  {i+1}. {row['feature']:20s}: {row['importance']:.3f}\")\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# 5.3 Model Comparison Bar Chart\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n[3/3] Model Comparison Chart...\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "models_to_plot = df_results[df_results['model'] != 'Dummy (Stratified)'].copy()\n",
        "models_to_plot = models_to_plot.sort_values('test_acc', ascending=True)\n",
        "\n",
        "y_pos = np.arange(len(models_to_plot))\n",
        "\n",
        "# Train accuracy (background, lighter)\n",
        "bars_train = ax.barh(y_pos, models_to_plot['train_acc'],\n",
        "                      color=STATUS_COLORS['light_purple'], alpha=0.4,\n",
        "                      label='Train Accuracy', edgecolor='white', linewidth=1.5)\n",
        "\n",
        "# Test accuracy (foreground, darker)\n",
        "bars_test = ax.barh(y_pos, models_to_plot['test_acc'],\n",
        "                     color=STATUS_COLORS['navy'], alpha=0.85,\n",
        "                     label='Test Accuracy', edgecolor='white', linewidth=1.5)\n",
        "\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(models_to_plot['model'], fontsize=11, weight='bold')\n",
        "ax.set_xlabel('Accuracy', fontsize=13, weight='bold', color=STATUS_COLORS['navy'])\n",
        "ax.set_title('STATUS Module - Model Comparison\\nTrain vs Test Accuracy',\n",
        "             fontsize=16, weight='bold', color=STATUS_COLORS['navy'], pad=20)\n",
        "ax.legend(loc='lower right', fontsize=11, framealpha=0.95)\n",
        "ax.set_xlim([0.85, 1.02])\n",
        "\n",
        "# Add value labels (test accuracy only)\n",
        "for i, test in enumerate(models_to_plot['test_acc']):\n",
        "    ax.text(test + 0.003, i, f'{test:.1%}',\n",
        "            va='center', fontsize=11, weight='bold',\n",
        "            color=STATUS_COLORS['navy'])\n",
        "\n",
        "# Highlight best model\n",
        "best_idx = models_to_plot['test_acc'].idxmax()\n",
        "best_pos = np.where(models_to_plot.index == best_idx)[0][0]\n",
        "ax.axhline(best_pos, color=STATUS_COLORS['purple'], linewidth=3, alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add \"BEST\" annotation\n",
        "ax.text(0.99, best_pos, '  ★ BEST', va='center', ha='right',\n",
        "        fontsize=10, weight='bold', color=STATUS_COLORS['purple'],\n",
        "        bbox=dict(boxstyle='round,pad=0.3', facecolor=STATUS_COLORS['light_purple'],\n",
        "                  alpha=0.3, edgecolor=STATUS_COLORS['purple']))\n",
        "\n",
        "# Grid styling\n",
        "ax.grid(axis='x', alpha=0.3, linestyle='--', color=STATUS_COLORS['royal_blue'])\n",
        "ax.set_axisbelow(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "comp_path = VIZDIR / 'model_comparison_v2.2.png'\n",
        "plt.savefig(comp_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
        "print(f\"  [OK] Saved: {comp_path}\")\n",
        "plt.close()\n",
        "\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 7 - MODEL SELECTION RATIONALE & SAVE**"
      ],
      "metadata": {
        "id": "kVAdtrdN8Qyl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccdlMR1X8Qyl",
        "outputId": "72c30727-432e-4c13-9ede-0a0b480632a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SECTION 6: MODEL SELECTION RATIONALE & SAVE\n",
            "================================================================================\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "BEST MODEL: Gradient Boosting\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Why Gradient Boosting over alternatives:\n",
            "[OK] Highest accuracy: 96.1% (vs 94.1% LR/RF)\n",
            "[OK] Highest F1-macro: 0.961\n",
            "[OK] Perfect Advanced precision: 100% (no false positives)\n",
            "[OK] Perfect Beginner recall: 100% (no missed beginners)\n",
            "[OK] Train-test gap: 0.039 (< 0.10 threshold, acceptable)\n",
            "[OK] Handles non-linear interactions (skip_rate × total_sets)\n",
            "\n",
            "Trade-offs accepted:\n",
            "[!]  Training time: 2.5s (vs 0.09s Logistic)\n",
            "[!]  Interpretability: Lower than Logistic Regression\n",
            "[!]  Gap slightly higher than Logistic (0.039 vs 0.027)\n",
            "\n",
            "Why NOT Logistic Regression (94.1%):\n",
            "[X] -2% accuracy (significant in 3-class problem)\n",
            "[X] Advanced precision 93.9% (6 false positives)\n",
            "[OK] But: Best generalization (gap 0.027), fastest (0.09s)\n",
            "\n",
            "Why NOT XGBoost (93.1%):\n",
            "[X] Gap 0.069 (too high, overfitting risk)\n",
            "[X] Lowest test accuracy among ensembles\n",
            "[X] Same training time as GB but worse performance\n"
          ]
        }
      ],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 6: MODEL SELECTION RATIONALE & SAVE\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SECTION 6: MODEL SELECTION RATIONALE & SAVE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"BEST MODEL: Gradient Boosting\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "print(\"\\nWhy Gradient Boosting over alternatives:\")\n",
        "print(\"[OK] Highest accuracy: 96.1% (vs 94.1% LR/RF)\")\n",
        "print(\"[OK] Highest F1-macro: 0.961\")\n",
        "print(\"[OK] Perfect Advanced precision: 100% (no false positives)\")\n",
        "print(\"[OK] Perfect Beginner recall: 100% (no missed beginners)\")\n",
        "print(\"[OK] Train-test gap: 0.039 (< 0.10 threshold, acceptable)\")\n",
        "print(\"[OK] Handles non-linear interactions (skip_rate × total_sets)\")\n",
        "\n",
        "print(\"\\nTrade-offs accepted:\")\n",
        "print(\"[!]  Training time: 2.5s (vs 0.09s Logistic)\")\n",
        "print(\"[!]  Interpretability: Lower than Logistic Regression\")\n",
        "print(\"[!]  Gap slightly higher than Logistic (0.039 vs 0.027)\")\n",
        "\n",
        "print(\"\\nWhy NOT Logistic Regression (94.1%):\")\n",
        "print(\"[X] -2% accuracy (significant in 3-class problem)\")\n",
        "print(\"[X] Advanced precision 93.9% (6 false positives)\")\n",
        "print(\"[OK] But: Best generalization (gap 0.027), fastest (0.09s)\")\n",
        "\n",
        "print(\"\\nWhy NOT XGBoost (93.1%):\")\n",
        "print(\"[X] Gap 0.069 (too high, overfitting risk)\")\n",
        "print(\"[X] Lowest test accuracy among ensembles\")\n",
        "print(\"[X] Same training time as GB but worse performance\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 7.1 - SAVE EBST MODEL**"
      ],
      "metadata": {
        "id": "eEPGPCdO8RPG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZyxQBeO8RPH",
        "outputId": "c6ddb89e-efed-4cd1-ba9d-e8b0427a2553"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "SAVING BEST MODEL\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[OK] Best model saved: models/status_best_model_v2.2.pkl\n",
            "  File size: 732.1 KB\n"
          ]
        }
      ],
      "source": [
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# Save Best Model\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"SAVING BEST MODEL\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "best_model_bundle = {\n",
        "    'model': gb_best,\n",
        "    'scaler': scaler,\n",
        "    'feature_names': feature_names,\n",
        "    'target_encoding': target_encoding,\n",
        "    'model_name': 'Gradient Boosting',\n",
        "    'test_accuracy': 0.960784,\n",
        "    'test_f1_macro': 0.960737,\n",
        "    'train_test_gap': 0.039216,\n",
        "    'hyperparameters': {\n",
        "        'n_estimators': 150,\n",
        "        'learning_rate': 0.1,\n",
        "        'max_depth': 4,\n",
        "        'min_samples_split': 10,\n",
        "        'min_samples_leaf': 4,\n",
        "        'subsample': 0.8\n",
        "    },\n",
        "    'feature_importance': feature_df.to_dict('records'),\n",
        "    'version': '2.2',\n",
        "    'timestamp': datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "model_path = MODELDIR / 'status_best_model_v2.2.pkl'\n",
        "\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump(best_model_bundle, f)\n",
        "\n",
        "print(f\"\\n[OK] Best model saved: {model_path}\")\n",
        "print(f\"  File size: {model_path.stat().st_size / 1024:.1f} KB\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 7.2 - SAVE MODEL INFO (JSON)**"
      ],
      "metadata": {
        "id": "kP1Uxszz8RWQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxMobgfg8RWQ",
        "outputId": "ac73d65e-ca00-4a69-f398-c4809c6787fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Model info saved: models/status_model_info_v2.2.json\n"
          ]
        }
      ],
      "source": [
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "# Save Model Info (JSON)\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "model_info = {\n",
        "    'model_name': 'Gradient Boosting',\n",
        "    'version': '2.2',\n",
        "    'date_trained': datetime.now().isoformat(),\n",
        "    'dataset': {\n",
        "        'n_train': len(X_train),\n",
        "        'n_test': len(X_test),\n",
        "        'n_features': len(feature_names),\n",
        "        'features': feature_names\n",
        "    },\n",
        "    'performance': {\n",
        "        'test_accuracy': 0.960784,\n",
        "        'test_f1_macro': 0.960737,\n",
        "        'test_precision_macro': 0.962698,\n",
        "        'test_recall_macro': 0.960784,\n",
        "        'train_test_gap': 0.039216,\n",
        "        'per_class': {\n",
        "            'Beginner': {\n",
        "                'precision': 0.971429,\n",
        "                'recall': 1.000000,\n",
        "                'f1': 0.985507\n",
        "            },\n",
        "            'Intermediate': {\n",
        "                'precision': 0.916667,\n",
        "                'recall': 0.970588,\n",
        "                'f1': 0.942857\n",
        "            },\n",
        "            'Advanced': {\n",
        "                'precision': 1.000000,\n",
        "                'recall': 0.911765,\n",
        "                'f1': 0.953846\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'hyperparameters': {\n",
        "        'n_estimators': 150,\n",
        "        'learning_rate': 0.1,\n",
        "        'max_depth': 4,\n",
        "        'min_samples_split': 10,\n",
        "        'min_samples_leaf': 4,\n",
        "        'subsample': 0.8\n",
        "    },\n",
        "    'feature_importance_top3': feature_df.head(3).to_dict('records'),\n",
        "    'training_time_sec': 2.479085\n",
        "}\n",
        "\n",
        "info_path = MODELDIR / 'status_model_info_v2.2.json'\n",
        "\n",
        "with open(info_path, 'w') as f:\n",
        "    json.dump(model_info, f, indent=2)\n",
        "\n",
        "print(f\"[OK] Model info saved: {info_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CELLA 8 - SUMMARY**"
      ],
      "metadata": {
        "id": "3JJINWFg8gQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# SECTION 7: SUMMARY\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STATUS MODELING v2.2 - COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n┌\" + \"─\"*78 + \"┐\")\n",
        "print(\"│\" + \" \"*26 + \"MODELING COMPLETE\" + \" \"*35 + \"│\")\n",
        "print(\"└\" + \"─\"*78 + \"┘\")\n",
        "\n",
        "print(\"\\nBEST MODEL\")\n",
        "print(\"-\"*80)\n",
        "print(\"Model:           Gradient Boosting\")\n",
        "print(\"Test Accuracy:   96.1%\")\n",
        "print(\"F1-macro:        0.961\")\n",
        "print(\"Train-Test Gap:  0.039 (< 0.10 [OK])\")\n",
        "\n",
        "print(\"\\nPER-CLASS PERFORMANCE\")\n",
        "print(\"-\"*80)\n",
        "print(\"Beginner:        Precision 97.1%, Recall 100%, F1 98.6%\")\n",
        "print(\"Intermediate:    Precision 91.7%, Recall 97.1%, F1 94.3%\")\n",
        "print(\"Advanced:        Precision 100%, Recall 91.2%, F1 95.4%\")\n",
        "\n",
        "print(\"\\nOUTPUT FILES\")\n",
        "print(\"-\"*80)\n",
        "print(f\"[OK] {model_path}\")\n",
        "print(f\"[OK] {info_path}\")\n",
        "print(f\"[OK] {comparison_path}\")\n",
        "print(f\"[OK] {cm_path}\")\n",
        "print(f\"[OK] {fi_path}\")\n",
        "print(f\"[OK] {comp_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STATUS MODULE COMPLETE (96.1% accuracy achieved!)\")\n",
        "print(\"=\"*80)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTA59sdN8il0",
        "outputId": "782cdf5e-3b0f-47ec-907e-5449ecc3dcce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STATUS MODELING v2.2 - COMPLETE\n",
            "================================================================================\n",
            "\n",
            "┌──────────────────────────────────────────────────────────────────────────────┐\n",
            "│                          MODELING COMPLETE                                   │\n",
            "└──────────────────────────────────────────────────────────────────────────────┘\n",
            "\n",
            "BEST MODEL\n",
            "--------------------------------------------------------------------------------\n",
            "Model:           Gradient Boosting\n",
            "Test Accuracy:   96.1%\n",
            "F1-macro:        0.961\n",
            "Train-Test Gap:  0.039 (< 0.10 [OK])\n",
            "\n",
            "PER-CLASS PERFORMANCE\n",
            "--------------------------------------------------------------------------------\n",
            "Beginner:        Precision 97.1%, Recall 100%, F1 98.6%\n",
            "Intermediate:    Precision 91.7%, Recall 97.1%, F1 94.3%\n",
            "Advanced:        Precision 100%, Recall 91.2%, F1 95.4%\n",
            "\n",
            "OUTPUT FILES\n",
            "--------------------------------------------------------------------------------\n",
            "[OK] models/status_best_model_v2.2.pkl\n",
            "[OK] models/status_model_info_v2.2.json\n",
            "[OK] models/status_model_comparison_v2.2.json\n",
            "[OK] visualizations/STATUS_Modeling_v2.2/confusion_matrix_best_model_v2.2.png\n",
            "[OK] visualizations/STATUS_Modeling_v2.2/feature_importance_best_model_v2.2.png\n",
            "[OK] visualizations/STATUS_Modeling_v2.2/model_comparison_v2.2.png\n",
            "\n",
            "================================================================================\n",
            "STATUS MODULE COMPLETE (96.1% accuracy achieved!)\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}